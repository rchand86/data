
Design pattern
prototype - Prototype design pattern is used when the Object creation is a costly and requires a lot of time and resources and you have a 
            similar object already existing. Prototype pattern provides a mechanism to copy the original object to a new object and then modify it according to our needs.
			EX: It would be easy to understand prototype design pattern with an example. Suppose we have an Object that loads data from database. 
			Now we need to modify this data in our program multiple times, so it’s not a good idea to create the Object using new keyword and load all the data again from database. 
			The better approach would be to clone the existing object into a new object and then do the data manipulation. 
			Prototype design pattern mandates that the Object which you are copying should provide the copying feature.
Facade-  	pattern hides the complexities of the system and provides an interface to the client using which the client can access the system

Observer -  
			pattern is used when there is one-to-many relationship between objects such as if one object is modified, its depenedent objects are to be notified automatically.
			Observer design pattern is useful when you are interested in the state of an object and want to get notified 
			whenever there is any change. In observer pattern, the object that watch on the state of another object are called Observer and the object that is being 
			watched is called Subject.
Strategy -  
			Strategy pattern is used when we have multiple algorithm for a specific task and client decides the actual implementation to be used at runtime.
			One of the best example of strategy pattern is Collections.sort() method that takes Comparator parameter. 
			Based on the different implementations of Comparator interfaces, the Objects are getting sorted in different ways.
Command - button clicked- encapusulate the req and pass to bean
			Command design pattern is used to implement loose coupling in a request-response model.
			In command pattern, the request is send to the invoker and invoker pass it to the encapsulated command object. 
			Command object passes the request to the appropriate method of Receiver to perform the specific action. 
			
Factory - A Factory Pattern says that just define an interface or abstract class for creating an object but let the subclasses decide 
		  which class to instantiate. In other words, subclasses are responsible to create the instance of the class.
		  In Factory pattern, we create object without exposing the creation logic to the client

Adapter pattern works as a bridge between two incompatible interfaces. 
This pattern involves a single class called adapter which is responsible for communication between two independent or incompatible interfaces.
The Components of the Adapter Design Pattern -:		  
		a) Target: first interface/calss with called by client.
		b) Adaptee: Second interface/class whihc is not compatible with first one.
		c) Adapter: This is the class that bridges the gap between the Target and Adaptee. 
		It implements the Target interface and internally uses an instance of the Adaptee to perform the desired operations.
		


JDBC interview questionsc

The execute() method: This method is used to execute SQL DDL statements, it returns a boolean value specifying weather the ResultSet object can be retrieved.
executeUpdate(): This method is used to execute statements such as insert, update, delete. It returns an integer value representing the number of rows affected.
executeQuery(): This method is used to execute statements that returns tabular data (example select). It returns an object of the class ResultSet.

Class.forName("oracle.jdbc.driver.OracleDriver");
Connection conn = DriverManager.getConnection(URL, username, passwd);
Statement sql_stmt = conn.createStatement();
ResultSet rset = sql_stmt.executeQuery
      ("SELECT empno, ename, sal, deptno FROM emp ORDER BY ename");


JWT makes secure communicate between two bodies
JWT is used for Authentication

oidc  server  authentication(valid) server  ms office 365
aothrozation role  access 

Lambda Limitation:
This has maximum execution time of 15 minutes, 
a memory range of 512 MB, 
and a default deployment package size of 50 MB, 
along with limits on payload sizes and concurrent executions. 

EC2(cloud computing)- virtual computer , that act as a web hosting server who serve the respose to the request ex netflix
	In simple words,ECS is a manager while EC2 instances are just like employees. All the employees (EC2) under this manager(ECS) can perform "Docker" tasks and 
	the manager also understands "docker" pretty well. So,whenever you need "docker" resources, you show up to the Manager. Manager already has status from 
	every employee(EC2) decides which one should perform the task.

	EC2 is a virtual machine on AWS and ECS is a container orchestration system on AWS. to run docker container or images on aws.  IAAS: infra as a service 
	
	To use ECS, you need to run your container into some virtual machines which EC2 is one of an option to provide that. CAAS: container as a service.
	   1 - cluster: group of task and service, resource infra  Farget / EC2
	   3 - Service: handle load balancea and scalability
	  2 -  Task:  your runnig container, lowest building block, running instance
     Create Cluster(infra Fargat/EC2 ) then task defenation(CPU,port,log, container/image location,OS) then deploy(service create, update and Run task : give name of task or combine,
	 replica,security group, public IP )
	 
	 launching the container we have 2 launch type option EC2 and Farget
	 ECS load container image on EC2 or fargat or deploy container on EC2 machine 
	 fargat is advance version of Lambda , get every thng to user 
	 
	 Cluster: Ec2: a group of container which run the task
	           Fargate : a group of task;
			   
			   EC2                                                       Farget
			   you have existing EC2 , great control                     setup time is less
                resource cost only to give                               user dont care of infra , pay for resource + duration.
			  
			  
						
IAM - access managment - It is used to manage user and there permission ro any API

ECS - service that makes it easy for you to deploy, manage, and scale containerized applications.
user -> Docker file -> ECR ->  task(ECS) one task(defain port whcih are open for containers) contain more than 1 containers (one container - UI app second - DB)

3rd highest salary in MySQL using LIMIT clause:

SELECT salary FROM Employee ORDER BY salary DESC LIMIT 2,1

Select Max(Salary) from emp where salary < (select max(salary) from emp where salary NOT IN(select max(salary) from emp))

SELECT MIN(salary) FROM (SELECT DISTINCT salary FROM employees ORDER BY salary DESC LIMIT 3) AS subquery;

SELECT TOP 1 salary FROM  (SELECT TOP 3 salary FROM Table_Name ORDER BY salary DESC) AS Comp ORDER BY salary ASC

Single table with emp_id and manager_id and get empName and manager Name.
SELECT emp.name,       
       manager.name       
FROM Employee AS emp
JOIN Employee AS manager 
  ON emp.emp_id = manager.manager_id

Edit: You should probably use LEFT JOIN if there are Emp with no manager.

SOLID stands for:
S - Single-responsiblity Principle 	   A class should have a single responsibility
O - Open-closed Principle              Classes should be open for extension, but closed for modification
L - Liskov Substitution Principle      This principle aims to enforce consistency so that the parent Class or its child Class can be used in the 
                                       same way without any errors.
                                       If S is a subtype of T, then objects of type T in a program may be replaced with objects of type S without
									   altering any of the desirable
									   properties of that program.
I - Interface Segregation Principle    This principle aims at splitting a set of actions into smaller sets so that a Class executes ONLY 
                                       the set of actions it requires.
									   Larger interfaces should be split into smaller ones
D - Dependency Inversion Principle     This principle aims at reducing the dependency of a high-level Class on the low-level Class by 
                                       introducing an interface.


spring-boot-starter-web
spring-boot-starter-data-jpa

<dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>org.springframework.boot</groupId>
                    <artifactId>spring-boot-starter-tomcat</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
</dependencies>

@SpringBootApplication: It is a combination of three annotations
@EnableAutoConfiguration: auto configure the spring app based on dependency added in pom for ex if h2 jar in pom but we did not configure any bean related to that then 
it will do auto.
, @ComponentScan: scan bean in basepackage so that it will visisble to IOC container.
, and @Configuration:


@SpringBootApplication
public class Application {

	public static void main(String[] args) {
		SpringApplication.run(Application.class, args);
	}



java 8
Date is Thread safe(setter not there) + new methods- localTIme, plus,  + time zone


		

Collection.stream().forEach()                                                                                Collection.forEach()
it first converts the collection to the stream and then iterates over the stream of collection.      Collection.forEach() uses the collections iterator.
If iteration is happening over the synchronized collection, then it does not lock the collection.	If iteration is happening over the synchronized collection, then it locks the collection and holds it across all the calls.
During structure modification in the collection, the exception will be thrown later.                  modification in the collection by using the collection.forEach() it will immediately throw an exception.
it does not execute in any specific order, i.e. the order is not defined.                              If always execute in the iteration order of iterable, if one is specified.
IF we want stream to perform action like lambda use this with no order.                                 If we wan the order and just iteration then use this.

The main difference between intermediate and terminal operations is that intermediate operations return a stream as a result and terminal operations return non-stream values 
like primitive or object or collection or may not return anything.
Pipeline of operations may contain any number of intermediate operations, but there has to be only one terminal operation, that too at the end of pipeline.
Intermediate operations are lazily loaded. When you call intermediate operations, they are actually not executed. They are just stored in the memory and executed 
when the terminal operation is called on the stream.
 As the names suggest, intermediate operations doesn’t give end result. They just transform one stream to another stream. On the other hand, terminal operations give end result.
Intermediate Operations :
map(), filter(), distinct(), sorted(), limit(), skip()

Terminal Operations :

forEach(), toArray(), reduce(), collect(), min(), max(), count(), anyMatch(), allMatch(), noneMatch(), findFirst(), findAny()

-------------------------------------------
Java 8 map() and flatMap() are two important methods of java.util.stream.Stream interface used for transformation or mapping operations. Both are intermediate operations. 
map() does only mapping, but flatMap() performs mapping as well as flattening/settle the result. Flattening means transforming data from Stream<Stream<T>> to Stream<T>. 
This is the main difference between map() and flatMap().

flatMap() operation takes Stream<Stream<T> as input and produces a result Stream of type R. It’s mapper function produces multiple values for each value of input stream 
and those multiple values are flattened into a result Stream<R>.
In short, it is used to convert a Stream of Stream into a list of values.

[ [2, 3, 5], [7, 11, 13], [17, 19, 23] ]  -->> [ 2, 3, 5, 7, 11, 13, 17, 19, 23 ] 
listOfLists.stream().flatMap(list -> list.stream()).forEach(System.out::println);

For example, suppose we have a instituteList where each Institute consists of two fields. One is its name and another one is its different locations wrapped in another List as below.

List<Institute> instituteList = new ArrayList<>();
         
instituteList.add(new Institute("IIM", Arrays.asList("Bangalore", "Ahmedabad", "Kozhikode", "Lucknow")));
instituteList.add(new Institute("IIT", Arrays.asList("Delhi", "Mumbai", "Kharagpur")));
instituteList.add(new Institute("NIFT", Arrays.asList("Hyderabad", "Mumbai", "Patna", "Bangalore")));

//Java 8 Map() : Get names of all institutes         
List<String> namesOfInstitutes = instituteList.stream().map(Institute::getName).collect(Collectors.toList());
o/p : [IIM, IIT, NIFT]

If we suppose to extract unique locations of all institutes, using map() will throw an error. Because, locations are itself wrapped in another List<String>
	
//Java 8 FlatMap() : Get unique locations of all institutes         
Set<String> locationsOfInstitutes = instituteList.stream().flatMap(institute -> institute.getLocations().stream()).collect(Collectors.toSet());
--------------------------------------------------------------------

java 8 vs 11:

Several new methods of String such as isBlank(), lines(), repeat(n)
	String newString = "-->".repeat(5);
	assertEquals("-->-->-->-->-->", newString);
	
	String str = " Geeks \n For \n Geeks \r Technical \r\n content \r writer \n Internship";
	// Generating stream of lines from string using line method
	Stream<String> lines = str.lines();
	lines.forEach(System.out::println);
	o/p: Geeks 
         For
It has a better garbage collection system --> G1GC. The default garbage collector in Java 11
In JAVA 11, there are various methods to work with the file such as writeString(), readString()
	Files.writeString(path, string(want to write in file), options)
	Files.readString(filePath) -->> read from file 
Java 11 added some new methods to the Pattern class --> asPredicate & asMatchPredicate
	Pattern.asPredicate will return true if any part of the input string matches the Regular expression. 
	Pattern.asMatchPredicate will return true if the entire input string matches the Regular expression. 
		 Pattern pattern = Pattern.compile("abc");
		 Predicate<String> asPredicate = pattern.asPredicate();
			// True, because abc is part of abc
			System.out.printf("asPredicate: abc: %s\n", asPredicate.test("abc"));
			// False, because abc is NOT part of 123
			System.out.printf("asPredicate: 123: %s\n", asPredicate.test("123")); // -> false

A major change in this version is that we don’t need to compile the Java source files with javac explicitly anymore:
$ javac HelloWorld.java
$ java HelloWorld 
Hello Java 8!

Instead, we can directly run the file using the java command:
$ java HelloWorld.java
Hello Java 11!

CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);
	

The Lambda expression is used to provide the implementation of an interface which has functional interface. 
It saves a lot of code. In case of lambda expression, we don't need to define the method again for providing the implementation. Here, we just write the implementation code.

Default methods,
Static methods in interface,

Basic
It is a static method which belongs to the interface only. We can write implementation of this method in interface itself
It is a method with default keyword and class can override this method

Method Name
Interface and implementing class , both can have static method with the same name without overriding each other.
We can override the default method in implementing class
4.
Use Case
It can be used as a utility method
It can be used to provide common functionality in all implementing classes


Java interface static method is similar to default method except that we can't override them in the implementation classes. 
		This feature helps us in avoiding undesired results incase of poor implementation in implementation classes.

Static methods can be used to define factory methods that create instances of classes that implement the interface. 



Method references,                     ------- use to refer method of the functional interface --------  (0) -> o.toString();   == Object:: toString
four type - :: staticMethod (Class), ::instanceMethod (Obj), className:: new


If you are not sure that object exist then use get() method bcoz it will return null object 
If you are sure that object exist then use load() method bcoz It will throw object not found exception ,   It always returns proxy object 

CREATE PROCEDURE citycount (IN country CHAR(3), OUT cities INT)
       BEGIN
         SELECT COUNT(*) INTO cities FROM world.city
         WHERE CountryCode = country;
       END//

CREATE TRIGGER person_bi BEFORE INSERT
ON person
FOR EACH ROW
IF NEW.age < 18 THEN
SIGNAL SQLSTATE '50001' SET MESSAGE_TEXT = 'Person must be older than 18.';
END IF; //

DROP FUNCTION IF EXISTS F_TEST //
CREATE FUNCTION F_TEST(PID INT) RETURNS VARCHAR
BEGIN
/*DECLARE VALUES YOU MAY NEED, EXAMPLE:
  DECLARE NOM_VAR1 DATATYPE [DEFAULT] VALUE;
  */
  DECLARE NAME_FOUND VARCHAR DEFAULT "";

    SELECT EMPLOYEE_NAME INTO NAME_FOUND FROM TABLE_NAME WHERE ID = PID;
  RETURN NAME_FOUND;
END;//	   


@RestController
@RequestMapping("/home")
public class IndexController {
    @RequestMapping(method = RequestMethod.GET)
    String get() {
        return "Hello from get";
    }

@RestController
@RequestMapping("/home")
public class IndexController {
    @RequestMapping(value = "/prod", produces = {"application/JSON"})
    @ResponseBody
    String getProduces() {
        return "Produces attribute";
    }	
	
	
Mockito is an open-source testing framework used for unit testing of Java applications. 
<dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
</dependency>	
  
   
@RunWith(MockitoJUnitRunner.class)
public class TestEmployeeManager {

	@InjectMocks
	EmployeeManager manager;

	@Mock
	EmployeeDao dao;

	//tests
}




CrudRepository and JPA repository both are the interface of the spring data repository library. 
	JPA extend crudRepository and PagingAndSorting repository
	Crud Repository is the base interface and it acts as a marker interface.
	JPA also provides some extra methods related to JPA such as delete records in batch and flushing data directly to a database.	
	Crud Repository provides only CRUD functions like findOne, saves, etc.
	Crud Repository
	PaginationAndSorting
	JpaRepo

What is dependency Injection?
The process of injecting dependent bean objects into target bean objects is called dependency injection.

Setter Injection: The IOC container will inject the dependent bean object into the target bean object by calling the setter method.
Constructor Injection: The IOC container will inject the dependent bean object into the target bean object by calling the target bean constructor.
Field Injection: The IOC container will inject the dependent bean object into the target bean object by Reflection API	

Why string immuatable?
String pool is possible bcoz of this,  save heap space
security- pass can be refer to 5 user and if someone change all will be effect.
multithreading use
can use as a key in hashMap.ie why it is use as a key

Rest vs GraphQL
Rest dis-
- Multiple Round Trips To Fetch Related Resources:
	E.g. imagine you’d like to request information from a post entity. At the same time you’d like to request information of post author (which is a different entity). Typically this is done by sending two request to the REST API (e.g. by using HTTP GET). First to retrieve the post object and second to retrieve the user object.
- Over Fetching / Under Fetching:
	By using endpoint mydomain.com/posts/:id we’re fetching data for a specific post. Each post might comprise the following properties: id, title, user, and body. You’ll always get back the complete set of data. There is no way to limit the response to only contain a subset of data like title and user.
GraphQL					REST
client driven           server driven           - Archetecture
development fast
Query mutation			GET,PUT POST DELETE

--------------------------------------------------------------------------------------
Distributing tracing in microservice  -   to know which service is fail while inter communication of services  -  it track the entire request chain of microservice
Spring clod Sleuth & zipkin(jar download and run) using these fault tolerence will resolve (Hystrix dashboard can also do the same but if one service has multiple instance 
running in that case Hystrix dashboard will fail) 

these two dependency will be added in all the microservice
-- tell to these services via app.yml file to know where this zipkin are running
     spring:
	  zipkin:
	    base-url= url of Zepkin 9411

Sleuth generate methdata with four attribute 1- Service name 2 traceid 3 span id 4 flag
Traceid will be the same for one request.
spain id is diff for diff services	 
-----------------------------------------------------------------------------
“CORS” stands for Cross-Origin Resource Sharing. It allows you to make requests from one website to another website in the browser, 
which is normally prohibited by another browser policy called the Same-Origin Policy (SOP).
----------------------------------------------------------------
How linked list is replaced with binary tree?

In Java 8, HashMap replaces linked list with a binary tree when the number of elements in a bucket reaches certain threshold. 
While converting the list to binary tree, hashcode is used as a branching variable. 
If there are two different hashcodes in the same bucket, one is considered bigger and goes to the right of the tree and other one to the left. 
But when both the hashcodes are equal, HashMap assumes that the keys are comparable, and compares the key to determine the direction so that some order can be maintained.
It is a good practice to make the keys of HashMap comparable.

static class Node<K,V> implements Map.Entry<K,V> {
final int hash;
final K key;
V value;
Node<K,V> next;
}

Initially, collisions are handled using a linked list.
If more than 8 entries collide in the same bucket, and the map has enough capacity, it switches to a red-black tree.
If entries are removed and drop below 6, it switches back to a linked list.
-------------------------------

The class must be declared as final so that child classes can’t be created.
Data members in the class must be declared private so that direct access is not allowed.
Data members in the class must be declared as final so that we can’t change the value of it after object creation.
A parameterized constructor should initialize all the fields performing a deep copy so that data members can’t be modified with an object reference.
Deep Copy of objects should be performed in the getter methods to return a copy rather than returning the actual object reference)

----------------------------------------------------------------------
How to resolve whitelabel error page in spring boot application?
1 - Custom Error Controller– where you will be implementing ErrorController  interface which is provided by SpringFramework and then overriding its getErrorPath() 
so that you can return a custom path whenever such type of error is occurred.
2 - By Displaying Custom error page– All you have to do is create an error.html page and place it into the src/main/resources/templates path. 
The BasicErrorController of of springboot will automatically pick this file by default.
3 - By disabling the whitelabel error page– this is the easiest way where all you need to do is server.error.whitelabel.enabled property to 
false in the application.properties file to disable the whitelabel error page.

How to create jar file in spring boot?
To create a jar file in spring boot you need to define your packaging file as jar in your pom.xml(if it is maven project).
Then just do maven build with specifying goals as package so that your application will start building.
Once the build is successful, just go into your Target folder and you can see .jar file generated for you application.

What is the need for Spring Boot DevTools?
This is one of the amazing features provided by Spring Boot, where it restarts the spring boot application whenever any changes are being made in the code. 
 Here, you don’t need to right-click on the project and run your application again and again. Spring Boot dev tools does this for you with every code change.
Dependency to be added is: spring-boot-devtools
The main focus of this module is to improve the development time while working on Spring Boot applications.

Stream: In this example the list.stream() works in sequence on a single thread with the print() 
List<String> list = Arrays.asList( "Hello ", 
                          "G", "E", "E", "K", "S!");        
        list.stream().forEach(System.out::print);
Output
Hello GEEKS!
		
Parallel stream leverage multi-core processors, which increases its performance. Using parallel streams, our code gets divide into multiple streams which can be 
executed parallelly on separate cores of the system and the final result is shown as the combination of all the individual core’s outcomes.

list.parallelStream().forEach(System.out::print);
Output
ES!KGEHello
Here we can see the order is not maintained as the list.parallelStream() works parallelly on multiple threads. If we run this code multiple times then 
we can also see that each time we are getting a different order as output but this parallel stream boosts the performance so the situation 
where the order is not important is the best technique to use.

Note: If we want to make each element in the parallel stream to be ordered, we can use the forEachOrdered() method, instead of the forEach() method.
list.parallelStream().forEachOrdered(System.out::print); 
Output
Hello GEEKS!


Need for Dependency Injection:
Suppose class One needs the object of class Two to instantiate or operate a method, then class One is said to be dependent on class Two. 
Now though it might appear okay to depend a module on the other but, in the real world, this could lead to a lot of problems, including system failure. 
Hence such dependencies need to be avoided.

public class TextEditor {
   private SpellChecker spellChecker;
   
   public TextEditor(SpellChecker spellChecker) {
      this.spellChecker = spellChecker;
   }
}
Here, the TextEditor should not worry about SpellChecker implementation. The SpellChecker will be implemented independently and will be provided 
to the TextEditor at the time of TextEditor instantiation. This entire procedure is controlled by the Spring Framework.

Spring IoC (Inversion of Control) Container is the core of Spring Framework. It creates the objects, configures and assembles their dependencies, 
manages their entire life cycle. The Container uses Dependency Injection(DI) to manage the components that make up the application. 
It gets the information about the objects from a configuration file(XML) or Java Code or Java Annotations and Java POJO class. These objects are called Beans. 
Since the Controlling of Java objects and their lifecycle is not done by the developers, hence the name Inversion Of Control.
There are 2 types of IoC containers:
 
BeanFactory     :    It doesn’t supports annotation based dependency    It doesn’t supports internationalization   It uses Lazy initialization
ApplicationContext : supports annotation based dependency  ,supports annotation based dependency   	It supports internationalization


Scaling Vertically – Adds additional computing power to an existing instance or node. A node is used to control a cluster of Docker containers, 
where more containers can be added to a cluster to scale vertically.
Scaling Horizontally – Does not add computing power to existing instances or nodes. Instead, it creates a new instance and evenly re-distributes any workloads 
between the group of instances.

Amazon CloudFront-------------

Improved performance: CloudFront is a content delivery network (CDN) that caches content at edge locations around the world. 
By serving S3 objects with CloudFront, you can improve the performance of your application by reducing latency and improving load times for users located far away from the S3 bucket.
Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront 
delivers your content through a worldwide network of data centers called edge locations.

If you're building high-performance websites and want static content such as images, JavaScript files and CSS files to download quickly to a client's browser, 
you might be disappointed with the slight delay experienced in Amazon S3. If you need faster download speeds, then you might be ready to push those same files over 
to Amazon CloudFront.

How CloudFront Works
CloudFront stores a cache in two different layers, the Edge location and Regional Edge Cache. The Regional Edge Cache runs on the 13 AWS Regions available across the globe. 
Due to the complexity involved in opening and maintaining regional data centers, 
AWS has also established smaller data centers called Edge locations that are used to help reduce latency. 

EC2,S3 --->  Regional Edge Cache ----> Edge location ---> client
					|
					|
					Edge Location --> Client




@Transactional(propagation = Propagation.PROPAGATION_REQUIRES_NEW)
public void doSomething() {
  // transactional method code
}

ISOLATION: It tells how the changes made by one trans and visiible to other trans running in parallel. default Isolation level depend on DB type. most relation DB default is 
			read-committed
READ_UNCOMMITTED:
	This isolation level allows a transaction to read data written by other transactions that have not yet been committed. 
	This can lead to dirty reads, where a transaction reads data later rolled back by another transaction. 
	NO READ and WRITE LOCK 

READ_COMMITTED: / ISOLATION_DEFAULT
	This isolation level ensures that a transaction can only read data that other transactions have committed. 
	This prevents dirty reads but does not prevent non-repeatable reads or phantom reads.
	Read: Shared LOck accured and release assoon as read is done.
	Write: Execulve Lock accoured and relaese at the end of the trans
	So if trans A read and release loack and other trans update this then trans A again read then it will get diffrent value, so repetable not solved

REPEATABLE_READ:
	This isolation level ensures that a transaction will always read the same data, even if other transactions are modifying that data concurrently. 
	This prevents dirty reads and non-repeatable reads but does not prevent phantom reads.
	Read: Shared LOck accured and release at the end of trans
	Write: Execulve Lock accured and relaese at the end of the trans
			So if trans A read and will not release lock then it wont be modified and in last if trans A read again before method end it will get the same value so non-rept solved.
			but other rows can update becuase lock on row only so phantom is still there.
	
SERIALIZABLE:
	This isolation level provides the highest isolation level, ensuring that transactions are executed in a serializable order. This prevents dirty reads, non-repeatable 
	reads, and phantom reads.
	Locking is same as repeatable-read + apply range lock + release at the end of trans.
	

Dirty read::::: one transction reads the uncommitted data of second transactions. if second tran get roll back then tran one called dirty read.
Non-repeatable read:::: read(fuzzy read) is that A transaction reads the same row serval time and there is a chance that it get diff values then it call non-repetable-read problem. 
					So same row's data is different between the 1st and 2nd reads because other transactions update the same row's data and commit at the same time(concurrently).

	Alice and Bob start two database transactions.
	Bob’s reads the post record and title column value is Transactions.
	Alice modifies the title of a given post record to the value of ACID.
	Alice commits her database transaction.
	If Bob’s re-reads the post record, he will observe a different version of this table row.

Phantom:::::
	Alice and Bob start two database transactions.
	Bob’s reads all the post_comment records associated with the post row with the identifier value of 1.
	Alice adds a new post_comment record(new row) which is associated with the post row having the identifier value of 1.
	Alice commits her database transaction.
	If Bob’s re-reads the post_comment records having the post_id column value equal to 1, he will observe a different version of this result set.

	So, while the Non-Repeatable Read applies to a single row, the Phantom Read is about a range of records which satisfy a given query filtering criteria.

Note: 
	DB Locking Type 
	1 - Shared/Read lock
	2 - Exclusive/Write Lock;  another tran cant read/write if somone already get this lock.,
	So if any row has shared lock then only it is free to read only.

When you declare your transactional method, you can specify the isolation level you want to use by setting the isolation attribute of the @Transactional annotation. For example:

@Transactional(isolation = Isolation.ISOLATION_REPEATABLE_READ)
public void doSomething() {
// transactional method code
}

Qualifier:
One of the most important annotations in spring is @Qualifier annotation which is used to eliminate the issue of which bean needs to be injected. 

@Component("fooFormatter")
public class FooFormatter implements Formatter {
 
    public String format() {
        return "foo";
    }
}

@Component("barFormatter")
public class BarFormatter implements Formatter {
 
    public String format() {
        return "bar";
    }
}

public class FooService {
     
    @Autowired
    @Qualifier("fooFormatter")
    private Formatter formatter;
}

By including the @Qualifier annotation, together with the name of the specific implementation we want to use, in this example Foo, 
we can avoid ambiguity when Spring finds multiple beans of the same type.



@ConditionalOnProperty:
we need to create some beans conditionally based on the presence and value of a configuration property.

@Service
@ConditionalOnProperty(name = "app.feature.new", havingValue = "false", matchIfMissing = true)

app.feature.new=true

partition key in kafka:
attaching a key to messages will ensure messages with the same key always go to the same partition in a topic. Kafka guarantees order within a partition, 
but not across partitions in a topic,so alternatively not providing a key - which will result in round-robin distribution across partitions - will not maintain such order.

Kafka is a messaging system built for high throughput and fault tolerance.
Kafka has a built-in patriation system known as a Topic.
Kafka Includes a replication feature as well.

Following are the traditional methods of message transfer:-

Message Queuing:- 
A point-to-point technique is used in the message queuing pattern. A message in the queue will be destroyed once it has been consumed, 
similar to how a message is removed from the server once it has been delivered in the Post Office Protocol. Asynchronous messaging is possible with these queues.
If a network problem delays a message's delivery, such as if a consumer is unavailable, the message will be held in the queue until it can be sent. 
This means that messages aren't always sent in the same order. Instead, they are given on a first-come, first-served basis, which can improve efficiency in some situations.

Publisher - Subscriber Model:- 
The publish-subscribe pattern entails publishers producing ("publishing") messages in multiple categories and subscribers consuming published 
messages from the various categories to which they are subscribed. Unlike point-to-point texting, a message is only removed once it has been consumed by all category subscribers.
Kafka caters to a single consumer abstraction that encompasses both of the aforementioned- the consumer group. 
Following are the benefits of using Kafka over the traditional messaging transfer techniques:
---------------------------------------------------------------------

			String str1 = "Bored";
			String str2 = "Robed";
			
			//Convert strings to lowercase
			str1 = str1.toLowerCase();
			str2 = str2.toLowerCase();								

			char[] str1charArray = str1.toCharArray();
             char[] str2charArray = str2.toCharArray();
             // sort the char array
             Arrays.sort(str1charArray);
             Arrays.sort(str2charArray);
             // if the sorted char arrays are same or identical
             // then the strings are anagram
             boolean result = Arrays.equals(str1charArray, str2charArray);
             if(result) 
             {
                System.out.println(str1 + " and " + str2 + " are anagrams of each other.");
              }

Write a java program to capitalize each word in string?

public class StringFormatter {  
public static String capitalizeWord(String str){  
    String words[]=str.split("\\s");  
    String capitalizeWord="";  
    for(String w:words){  
        String first=w.substring(0,1);  
        String afterfirst=w.substring(1);  
        capitalizeWord+=first.toUpperCase()+afterfirst+" ";  
    }  
    return capitalizeWord.trim();  
}  
}  




Heap, method area , PC register,native method stack, JVM stack

The heap is generally divided into two parts. That is:  

Young Generation(Nursery): All the new objects are allocated in this memory. Whenever this memory gets filled, the garbage collection is performed. 
	This is called as Minor Garbage Collection.
Old Generation: All the long lived objects which have survived many rounds of minor garbage collection is stored in this area. 
	Whenever this memory gets filled, the garbage collection is performed. This is called as Major Garbage Collection.

PermGen Memory: This is a special space in java heap which is separated from the main memory where all the static content is stored in this section. 
	Apart from that, this memory also stores the application metadata required by the JVM. 
	Metadata is a data which is used to describe the data. Here, garbage collection also happens like any other part of the memory.
	The biggest disadvantage of PermGen is that it contains a limited size which leads to an OutOfMemoryError. 
	The default size of PermGen memory is 64 MB on 32-bit JVM and 82 MB on the 64-bit version. 
	It is Contiguous Java Heap Memory.  It is removed from java 8.

MetaSpace :  It is introduced in Java 8.,  Native Memory(provided by underlying OS).


We can use the Optional class to wrap our data and avoid the classical null checks and some of the try-catch blocks.

A Stream, which represents a sequence of data objects .It accepts Functional Interfaces so that lambdas can be passed

Arraylist
It is used to store only similar types of data.	
Less memory is used.
memory allcation at compile time	

linklist
insert delete is fast
It is used to store any types of data.
More memory is used.
memory allocation at run time


type Query {
  products(match : String) : [Product]   # a list of products
}

type Product {
  id : ID
  name : String
  description : String
  cost : Float
  tax : Float
  launchDate(dateFormat : String = "dd, MMM, yyyy') : String
}
The Query.products field has a data fetcher, as does each field in the type Product.

Data fetchers are also known as “Resolvers” in many graphql implementations”. Resolver function is the place where graphql resolves the type or the field and gets its value 
from the configured resources like a database or other APIs or from cache etc and returns data back to user/caller.


----------------------------------------------Spring boot authentication Using jwt-------------------------------------------------------------

JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.
the information in JWTs can be verified and trusted because it is digitally signed using a secret key or a public/private RSA key pair.
{
  "sub": "1234567890",
  "name": "Nam Ha Minh"
  "iss": "codejava.net",
}
Here, the claim names used are sub (subject), name (full name) and iss (issuer), which are of type registered claims - defined by IANA (Internet Assigned Numbers Authority). 
There are also public and private claims, but it’s recommended to use registered claims for interoperability.

@Controller–   // Sample controller which will hit once we have a valid Token
{
@RequestMapping("/user")
public String hello(){
return "Hello"
}
}

@EnableWebSecurity
public class AppSecurityConfig  extends WebSecurityConfigurerAdapter {
@Autowired
MyUserDetailsService myUSerDetials;

@override
configure(AutheneticationManagerBuilder at){
at.userDetailsService(myUSerDetials);
}

@override
configure(HttpSecurity http){  // this will disable the authenticate for /auth method or first method which is creating th JWT token.
http.csrf().disable().authorizeRequests().anyMatcher("/auth").permitAll().anyRequest().authenticate();
}

@Bean
public PasswordEncoder passEncod(){
NoOpPasswordEncoder.instance()// tell to security framework no need of hashing for password
}
}

@Service
MyUserDetailsService inplements UserdetailsService(from Spring security){
@overrride
UserDetails loadUserByUserName(String userName){
return new User("ravi","ravi"); // this User class from Security calss// hard code/DB
 // if we connect DB the from list filter the given userName and create User Object and return;
}
}

If we run this, spring securtiyt with ask for user pass pop up.because we added the springSecurity dependency, so this is without JWT

With JWT:::::::::::::::
add dependency jjwt-jsonwebtoken   ------- create jwt token+ validate

Create/use exsisting JwtTokenUtils class which has some methods like:
public String generateToken(UserDetails userDetail){
claims= hasMap
return createToken(claims,userDetail.getUserName())
}

private createToken(map claim, String subject){
builder.Subject(subject).setIssuer(currentTIme).setExpiration(time).signWith(SignatureAlgo.HS256,Secret_key)
}
validateTOKEN()
IStoKENeXPIRED()

Step 1....................
authenticate api endpoint that:
accept user and pass and return JWT token

Create 2 class/model optional: Authrequest(user and pass) and AuthRes(token jwt)

Controller add one more : 
@Autowired
MyUserDetailsService myUSerDetials;
@Authwire
JwtTokenUtils jwtTokenUtils;

@Authwire
AuthenticationManger authManager;

@RequestMapping("/auth", method = POSt)
public ResponseEntity createToekn(@RequestBody Authrequest req){
try{
	authManager.authenticate(new UsernamePasswordAuthenticationToken(req.getUserName, req.getPass);  // AuthenticationManger impl calss is ProviderManager - 
	//this call the userDetialService and passwordEncoder to match user and pass is correct.
}catch(){
	user or pass incorrect;
}
UserDetails userDetails = myUSerDetials.loadUserByUserName(req.getUserName)
String token = jwtTokenUtils.generateToken(userDetails)
return ResponseEntity.ok(new Authres(token);
}

@override
configure(HttpSecurity http){  // this will disable the authenticate for /auth method.
http.csrf().disable().authorizeRequests().anyMatcher("/auth").permitAll().anyRequest().authenticate();
}


RUN:::::::::::::::::::::::::
POST :  localhost:8080/user/auth (pass user and pass) -   this will return the token same as IDP in bnsg

Validate the Token and process subsequence request: 

TO intercept the request and look to header and validate the token we need the filter : lot o filter already there so we need to extends one of them and implement your owm filter method

Class: JwtRequestFIlter()extends OncePerRequestFIlter{  //intercept every res just once 
@Autowired
MyUserDetailsService myUSerDetials;
@Authwire
JwtTokenUtils jwtTokenUtils;

@Override
protected doFIlterInternal(HttpServletRequest req, HttpServletResponse res, FilterChain chain){ // chain : option to pass to onther filter
//This filter work:  if token is valid then it get the UserDetials and save into SecurityContext
String header = req.getHeader("Authorization");
// remove Bearer from header: and get jwt
usename = jwtTokenUtils.extractUserName(jwt)
if(useranme != null and SecurityContextHolder.getCOntext().getAuthintaction() == null){
UserDetails userDetails = myUSerDetials.loadUserDetails(username);
  if(jwtTokenUtils.validateTOKEN(jwt, userDetails){
  // Set in SecurityContextHolder
  UsernamePasswordAuthenticationToken aa = new UsernamePasswordAuthenticationToken(userDetails,null,userDetails.getAuthorities());
  aa.setDetails(new WebAuthenticationDetailsSource().buildDetails(req);
  SecurityContextHolder.getCOntext().setAuthentication(aa);
  }
}
chain.doFilter(req,res)


}


@Authwire
JwtRequestFIlter JwtRequestFIlter;

@override
configure(HttpSecurity http){  // this will disable the authenticate for /auth method.
http.csrf().disable().authorizeRequests().anyMatcher("/auth").permitAll().anyRequest().authenticate().add().sessionManagement().sessionCreationpolicy(SessionCreationPolicy.STATELESS);
	 // tell to Spring Security dont manager/create session. bcoz by JWT we are making it STATELESS, so if Spring security will not create session then it need some thing which 
	// will set the SecurityContext on each request.
http.addFilterBefore(JwtRequestFIlter,UsernamePasswordAuthenticationFilter.class);  // call jwt filter before UsernamePasswordAuthenticationFilter

}


RUN : the GET.............
For subsequence req like GET:
add Header: Authorization - Bearer Token from first URL(/auth)

-----------------------------------------------------------Authorization  + Authentication--------------------------------------------------------------

Note : Ravi as a admin which feature he can access, raj as a user which feature can access

Spring 2.x

//Authentication
	@override
	protected configure(AutheneticationManagerBuilder au)
		au.inMemoryAuthentication().withUser("ravi").password(passwordEncoder().encode("pwd").roles("USER");
		au.inMemoryAuthentication().withUser("raj").password(passwordEncoder().encode("pwd").roles("ADMIN");
	}

//Autherization
	configure(HttpSecurity http){
		// same
	}

3.x------ WebSecurityCOnfigurerAdapter is depricated and we need to create Bean for UserDetailsService(coming from spring sceurity)

@EnableWebSecurity
@EnableMethodSecurity    // for method level role/authorization
public class AppSecurityConfig{
	@Authwire
	JwtAuthFilter jwtAuthFilter;
	
	//Authentication        validate the given user and password
	@Bean
	public UserDetailsService userDetails(PasswordEncoder encoder){
		UserDetails admin = UserDetails.withUserName("ravi").password(encoder.encode("pwd")).roles("ADMIN").build();
		UserDetails user = UserDetails.withUserName("raj").password(encoder.encode("pwd")).roles("USER").build();
		
		return new InMemoryUserDetailsManager(admin,user);
	}
	// with actual DB	
		// create a class(UserInfoDetialsService) which implements the UserDetailsService which override the UserDetails loadUserByUserName(String userName)
		// get DBUSer and convert to UserDetails object and return ; for this we can create a new class that will implement the UserDetails and override the methods of Userdetails
		// and return the property gettign from DB.
		// <GrantedAuthority> - role property inside UserDetails. 
		//Settign roles from DB to UserDetails property : 
		// List <GrantedAuthority> authorities = Arrays.Stream(dbUser.getROles().split(",").map(SimpleGrantedAuthority:: new).collect(Collectors.toList());
	   //return new UserInfoDetialsService();
	


	@Bean
	public PasswordEncoder passEncod(){
	 return new BCryptPasswordEncoder(); // encrypt the plain password
	}
	
	This AuthenticationProvider will talk to UserDetails and generate the UserDetails object and set to authentication.
	@Bean
	public AuthenticationProvider provider(){
	 DaoAuthenticationProvider aa = new DaoAuthenticationProvider(); // this need who is the UserdetailsService and PasswordEncoder
	 aa.setuserDetailsService(userDetails())
	 aa.setpasswordEncoder(passEncod())
	}
	
	}

	//Authorization   it authorize the correct url   // same as config in 2.x

	@Bean
	public SecurityFIlterChain(HttpSecutiy http){
	//same
		return http.csrf().disable().authorizeHttpRequests().requestMatcher("/auth").permitAll().and().authenticate().and().formLogin().and().build();
		
		// this is fot jwt token, which mnanage session + run filter BEFORE OTHER.
		return http.csrf().disable().authorizeHttpRequests().requestMatcher("/auth").permitAll().and().authenticate().and().sessionManagement()
		.sessonCreationPolicy(SessionCreationPolicy.STATELESS)
		.and().authenticationProvider(provider())
		.addFilterBefore(jwtAuthFilter, UsernamePasswordAuthenticationFilter.class).build(); // after add filter for token
	}
}
// Note : if we run this then. for "/auth" url security will not ask for user/pass popup but if we give another url then it will show the login popup.

---------Method level authorization----------------Inside COntroller
Note : 2 thing add 1- @PreAuthorize and @EnableMethodSecurity inside config class.

Controller ----------------------------------

@PostMapping("/update")							
@PreAuthorize("hasAuthority('ROLE_ADMIN')")  ------ for this we nee to tell spring security that we have implement the method level authorization, for that we nee to enable @EnableMethodSecurity with  @EnableWebSecurity
publice updateProduct(){
}

@GetMapping("/all")
@PreAuthorize("hasAuthority('ROLE_USER')")
publice getProduct(){
}

Note: if we run this then raj will not access to updateProduct() bcoz it is user role.

----------------Note: with above approach or without JWT token/OIDC login user/pass window will come for alll the URL/request, to get rid of this we use JWT 

JWT ---- 3 component Header(type of algo) , payload(subjectc,name) and signature(combination of other 2 encoded base64)
Note if someone know your name then it wont able to get the details bcoz it is secure by header and signature

Controller.java-----------------------------

	@Authwire
	AuthenticationManger authManager;   // explicityl define the @Bean for spring 3.x(and get it from AuthenticationConfiguration) in 2.x no need 

	@RequestMapping("/auth", method = POSt)
	public String createToekn(@RequestBody Authrequest req){

	// first validate the user/pass then generate the token or else it will generate token for all;
	 Authenticate auth = authManager.authenticate(new UsernamePasswordAuthenticationToken(req.getUserName, req.getPass)  // AuthenticationManger impl calss is ProviderManager - 
	 if(auth.isAuthenticated()){                                                            //this call the userDetialService and passwordEncoder to match user and pass is correct.
	  // generate and return token
		 return Jwts.builder().setClaims(claims empty hashmap).setSubject(useranme).setissueAt(time).setExpiration(time)
			 .signWith(getSignKey(), SignatureAlogrithim.HS256) // signKey(3rd part of jwt) and SignatureAlogrithim(first part), which algo you are using to sign this Key 
				.compact(); 
	 }else{
	   throw exception ; invalid user;
	 }
	}

	// this will give signKey based on our own Secret
	private Key getSignKey(){ 
		byte[] keyBytes = Decoders.BASE64.decode(SECRET); // SECRET: can copy/create encryc]ption key geneartion 256 bit, 512,128,64 etc (ex: SJDKLFJKDSJ2525JFKLJDS87687890890809890)
		Keys.hmacShaKeyFor(keyBytes); Keys from JWT package.
	}
	
	
	
	
}
NOTE:  So if fake user knows your user anme and pass and generate the JWT token from jwt.io then also it whont able to access because he dont know about header and signnate part
       which we implement as per our need. HS512, HS526 and our encry key generation(256,512) can be diff form jwt.io 
NOTE :   jjwt-api, jjwt-impl, jwt-jackson

// intercept the request and get header and validate the token, we need filter.
@Component
class JwtAuthFilter wxtends OncePerRequestFIlter{
	@Authwire
	JwtTokenUtils jwtTokenUtils;

	@Override
	protected doFIlterInternal(HttpServletRequest req, HttpServletResponse res, FilterChain chain){ // chain : option to pass to onther filter
	//This filter work:  if token is valid then it get the UserDetials and save into SecurityContext
	String header = req.getHeader("Authorization");
	// remove Bearer from header: and get jwt
	usename = jwtTokenUtils.extractUserName(jwt)
	
	if(useranme != null and SecurityContextHolder.getCOntext().getAuthintaction() == null){
		  UserDetails userDetails = myUSerDetials.loadUserDetails(username); // myUSerDetials or inject UserInfoDetialsService for DB 3.x
		  
		  if(jwtTokenUtils.validateTOKEN(jwt, userDetails){  // validateTOKEN() methods ; we can get from JWTUtisl or create own
			  // Set in SecurityContextHolder
			  UsernamePasswordAuthenticationToken aa = new UsernamePasswordAuthenticationToken(userDetails,null,userDetails.getAuthorities());
			  aa.setDetails(new WebAuthenticationDetailsSource().buildDetails(req);
			  SecurityContextHolder.getCOntext().setAuthentication(aa);
		  }
	}
	chain.doFilter(req,res)
	
// extractUserName() and others methods ; we can get from JWTUtisl or create own

   extractAllClaims(token){ // this method call from extractUserName() and validateTOKEN()
     return JWTs.parserBuilder.setSiginigKey(getSignKey()).build().parseClaimsJwt(token).getBody();  // getSignKey() we define above while generating
   }
}

NOTE   :    Add in config so that above filter will run:

// this is fot jwt token, which mnanage session + run filter BEFORE OTHER.
		return http.csrf().disable().authorizeHttpRequests().requestMatcher("/auth").permitAll().and().authenticate().and().sessionManagement()
		.sessonCreationPolicy(SessionCreationPolicy.STATELESS)
		.and().authenticationProvider(provider())
		.addFilterBefore(jwtAuthFilter, UsernamePasswordAuthenticationFilter.class).build(); // after add filter for token
		
		
		
--------------------------------------------------------------OIDC----------One of the key features of Spring Security 5 was the native support for OAuth2 and OIDC.--------------------------------------------------------------------------		


OpenID connect: it is used for single signon, in big company where they have multiple applicatons like product , order payment etc so no need to sign on each application.
Dependencies: three projects with 3 main dependency
1 - OAuth2 Authorization Server   -- which contain the user name pass to auth
2- OAuth2 Resource Server         -- resources which are restricted to use/ main user data
3- OAuth2 Client Server           -- client whcih call the resources.


spplicaton.yml--
// this code means that resource server user auth server to validate the client info
spring:
  security.oauth2:
    resourceserver:
	  jwt:
	    issuer-uri: http://auth-server:8081  // this is the auth server which validate the client info.


---------------------------------------------using okta auth server :  -------------------------    
dependency: oauth2, security, okta, 

- need to create account on OKTA auth server site and get the clent id and secret id
- create app here
	- create base URL and redirect URL(from where request will come to auth server and redirect to whihc URL(our springboot app))
- It will provide the client_id and Client_secret -  this will use our application to communicate springboo to okta auth server.

Controller–--------- Note: Principle class is from spring security which has user name which we se inside okta (emailid)
@EnableOAuth2Sso



application.property-------------------
okta.oauth2.client-id =  from okta
okta.oauth2.secret-id = from okta

okta.oauth2.issuer=  // get it from okta -> API ->default endpoint: https://dev.12344.okta.com/default
server.port= 9090  // this is the port which we gave in Okta auth sever while creating application inside okta
spring.main.allow-bean-definition-overriding = true

If we run this springboot url : and hit 9090 then it will auto redirec to okta site (https://dev.12344.okta.com/login) with popup username and pass
and same user and pass need to give which we set at the time of sign on to okta.
okta will verify the user with client and secret and rediret to springboot application controller

---------------------------------------------------------------------------------------Auth Server-----------------------------------

 First the scenario for OpenID:

User wants to access his account on example.com
example.com (the “Relying Party” in OpenID lingo) asks the user for his OpenID
User enters his OpenID
example.com redirects the user to his OpenID provider
User authenticates himself to the OpenID provider
OpenID provider redirects the user back to example.com
example.com allows the user to access his account
And now the scenario for OAuth:

User is on example.com and wants to import his contacts from mycontacts.com
example.com (the “Consumer” in OAuth lingo) redirects the user to mycontacts.com (the “Service Provider”)
User authenticates himself to mycontacts.com (which can happen by using OpenID)
mycontacts.com asks the user whether he wants to authorize example.com to access his contacts
User makes his choice
mycontacts.com redirects the user back to example.com
example.com retrieves the contacts from mycontacts.com
example.com informs the user that the import was successful
From those scenarios we can see that OpenID is about authentication (i.e. I can identify myself with an url) whereas OAuth is about authorization 
(i.e. I can grant permission to access my data on some website to another website, without providing this website the authentication information for the original website).

--------------------------------------------------------spring.main.allow-bean-definition-overriding=true---------------------------------------------------------
@Configuration
public class TestConfiguration1 {
@Bean
    public TestBean1 testBean(){
        return new TestBean1();
    }

@Configuration
public class TestConfiguration2 {
@Bean
    public TestBean2 testBean(){
        return new TestBean2();
    }
	
Because bean defination is with same name so app will gave error: 

Invalid bean definition with name 'testBean' defined in ... 
... com.baeldung.beandefinitionoverrideexception.TestConfiguration2 ...
Cannot register bean definition [ ... defined in ... 
... com.baeldung.beandefinitionoverrideexception.TestConfiguration2] for bean 'testBean' ...
There is already [ ... defined in ...
... com.baeldung.beandefinitionoverrideexception.TestConfiguration1] bound.

Solution : 
1 - Changing Method Names
2 - @Bean Annotation
	@Bean("testBean1")
	public TestBean1 testBean() {
		return new TestBean1();
	}
	
	@Bean("testBean2")
	public TestBean1 testBean() {
		return new TestBean2();
	}
3- spring.main.allow-bean-definition-overriding=true

--------------------------------------------------------Reflection api-------------------------------------------------------------------------------------------
Reflection is an API that is used to examine or modify the behavior of methods, classes, and interfaces at runtime, So Through reflection, we can invoke methods at runtime.
The required classes for reflection are provided under java.lang.reflect package 

One of the most valuable and basic uses of reflection is to find out what methods are defined within a class.
	Class c = Class.forName(args[0]);
	Method m[] = c.getDeclaredMethods();


The Reflection API is mainly used in: 
IDE (Integrated Development Environment) e.g., Eclipse, MyEclipse, NetBeans etc.
Debugger
Test Tools etc.

Where to use:
For example, say you have an object of an unknown type in Java, and you would like to call a 'doSomething' method on it if one exists. 
Java's static typing system isn't really designed to support this unless the object conforms to a known interface, but using reflection, 
your code can look at the object and find out if it has a method called 'doSomething' and then call it if you want to.

So, to give you a code example of this in Java (imagine the object in question is foo) :

Method method = foo.getClass().getMethod("doSomething", null);
method.invoke(foo, null);

There is a class in Java named Class that keeps all the information about objects and classes at runtime. The object of Class can be used to perform reflection.
In order to reflect a Java class, we first need to create an object of Class.

And, using the object we can call various methods to get information about methods, fields, and constructors present in a class.

There exists three ways to create objects of Class:

1. Using forName() method:  Class a = Class.forName("Dog");

2. Using getClass() method: Dog d1 = new Dog();   -->> Class b = d1.getClass();
3. Using .class extension:  Class c = Dog.class;

Now that we know how we can create objects of the Class. We can use this object to get information about the corresponding class at runtime.

import java.lang.Class;
import java.lang.reflect.*;

class Animal {
}

// put this class in different Dog.java file
public class Dog extends Animal {
  public void display() {
    System.out.println("I am a dog.");
  }
}

// put this in Main.java file
class Main {
  public static void main(String[] args) {
    try {
      // create an object of Dog
      Dog d1 = new Dog();

      // create an object of Class
      // using getClass()
      Class obj = d1.getClass();

      // get name of the class
      String name = obj.getName();
      System.out.println("Name: " + name);

      // get the access modifier of the class
      int modifier = obj.getModifiers();

      // convert the access modifier to string
      String mod = Modifier.toString(modifier);
      System.out.println("Modifier: " + mod);

      // get the superclass of Dog
      Class superClass = obj.getSuperclass();
      System.out.println("Superclass: " + superClass.getName());
    }

    catch (Exception e) {
      e.printStackTrace();
    }
  }
}

Like methods, we can also inspect and modify different fields of a class using the methods of the Field class.
Field field1 = obj.getField("type");
int mod = field1.getModifiers();
// convert the modifier to String form
String modifier1 = Modifier.toString(mod);


-------------------------------------------------------------------------------------AWS--------------------------------------------------------------
AWS regions: are separate geographical areas, like the US-West 1 (North California) and Asia South (Mumbai).
availability zones: are the areas that are present inside the regions. These are generally isolated zones that can replicate themselves whenever required.
	AWS regions has multiple isolated locations called availability zone 
	
Auto-scaling: is a function that allows you to provision and launch new instances whenever there is a demand. 
It allows you to automatically increase or decrease resource capacity in relation to the demand.

What is geo-targeting in CloudFront?
Geo-Targeting is a concept where businesses can show personalized content to their audience based on their geographic location without changing the URL.
In Amazon CloudFront we can detect the country from where end users are requesting our content. This information can be passed to our Origin server by Amazon CloudFront. 
It is sent in a new HTTP header.
Based on different countries we can generate different content for different versions of the same content. 
These versions can be cached at different Edge Locations that are closer to the end users of that country.
In this way we are able to target our end users based on their geographic locations.

What is geo restriction in CloudFront?
The CloudFront geographic restrictions feature lets you control distribution of your content at the country level for all files that you're distributing with a 
given web distribution.

What services can be used to create a centralized logging solution?
The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use Amazon Elastic Search to visualize them.

AWS CloudTrail
This is a service that provides a history of the AWS API calls for every account. 
The best part about this service is that it enables you to configure it to send notifications via AWS SNS when new logs are delivered.

DDoS is a cyber-attack in which the perpetrator accesses a website and creates multiple sessions so that the other legitimate users cannot access the service. 
The native tools that can help you deny the DDoS attacks on your AWS services are:
AWS Shield
AWS WAF
Amazon Route53
Amazon CloudFront
ELB
VPC

Creating subnets means dividing a large network into smaller ones.

Is there a way to upload a file that is greater than 100 megabytes on Amazon S3?
Yes, it is possible by using the multipart upload utility from AWS.

The maximum number of S3 buckets that can be created is 100.

The classic load balancer is used for simple load balancing of traffic across multiple EC2 instances. ex - instances in one availability zone 
While the application load balancing is used for more intelligent load balancing, based on the multi-tier architecture or container-based architecture of the application. 
Application load balancing is mostly used when there is a need to route traffic to multiple services. ex- instances within multiple availability zone.

AWS Glacier is an extremely low-cost storage service offered by Amazon that is used for data archiving and backup purposes

What are Key-pairs?
An Amazon EC2 uses public key cryptography which is used to encrypt and decrypt the login information. In public key cryptography, 
the public key is used to encrypt the information while at the receiver's side, a private key is used to decrypt the information. 
The combination of a public key and the private key is known as key-pairs. Key-pairs allows you to access the instances securely.


CloudFront is a computer delivery network which consists of distributed servers that delivers web pages and web content to a user based on the geographic locations of a user

The minimum size of an object that you can store in S3 is 0 bytes and the maximum size of an object that you can store in S3 is 5 TB.

Elastic Block Store is a service that provides a persistent block storage volume for use with EC2 instances in aws cloud.

AMI stands for Amazon Machine Image. It is a virtual image used to create a virtual machine within an EC2 instance. Yes, an AMI can be shared.

EIP (Elastic IP address) is a service provided by an EC2 instance. It is basically a static IP address attached to an EC2 instance. 
This address is associated with your AWS account not with an EC2 instance. 
You can also disassociate your EIP address from your EC2 instance and map it to another EC2 instance in your AWS account.

S3 bucket can be secured in two ways:
ACL (Access Control List)    ACL is used to manage the access of resources to buckets 
Bucket Policies              Bucket policies are only applied to S3 bucket

Following are the different types of instances:
General Purpose Instance type
Memory Optimized Instances	
Storage Optimized Instances

Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of aws cloud. 

VPC stands for Virtual Private Cloud. It is an isolated area of the AWS cloud where you can launch AWS resources in a virtual network that you define. 
It provides a complete control on your virtual networking environment such as selection of an IP address, creation of subnets, configuration of route tables and network gateways.

NAT stands for Network Address Translation. It is an aws service that enables to connect an EC2 instance in private subnet to the internet or other AWS services.

You can control the security to your VPC in two ways:
Security Groups
	It acts as a virtual firewall for associated EC2 instances that control both inbound and outbound traffic at the instance level. To know more about Security Groups, 
	It is associated with an EC2 instance. It is the first layer of defense.
	Security Group is applied to an instance only when you specify a security group while launching an instance.
	NACL has applied automatically to all the instances which are associated with an instance.
Network access control lists (NACL)
	It acts as a firewall for associated subnets that control both inbound and outbound traffic at the subnet level. It is associated with a subnet.
	It is the second layer of defense.

When large section of IP address is divided into smaller units is known as subnet.
A Virtual Private Cloud (VPC) is a virtual network provided to your AWS account. When you create a virtual cloud, 
you need to specify the IPv4 addresses which is in the form of CIDR block. After creating a VPC, you need to create the subnets in each availability zone. 
Each subnet has a unique ID. When launching instances in each availability zone, it will protect your applications from the failure of a single location.

AWS Shield is the service that protects against DDoS (Distributed Denial of Service) attacks on AWS applications.



CI/CD
frequent changes/release/integration to prod with minor changes so client can see more update very frequently, releaseing smaller changes very offen
pre build testing script help ensure compliacne and quailty
build automation 
before deployment o prod we can introduce multiple step like code coerage code quality test-case so can find problem in early stages

CI : (develop the artifacts)manage the codeing like code coverage report + generating automatic builds once build done it move to CD 
CD : (manage the artiface and making them in a running state) communicate CI pipline and deploy the code to prod

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
sec,min,hour,day of month, month,day of week
0 0 6 * * ?   --->>> 6 am daily
---------------------------------------
Comparable	Comparator
Present in java.lang package	Present in java.util package
Elements are sorted based on natural ordering	Elements are sorted based on user-customized ordering
Provides a single method called compareTo()	Provides to methods equals() and compare()
Modifies the actual class	Doesn’t modifies the actual class

failfast	failsafe
Doesn’t allow modifications of a collection while iterating	Allows modifications of a collection while iterating
Throws ConcurrentModificationException	Don’t throw any exceptions
Uses the original collection to traverse over the elements	Uses a copy of the original collection to traverse over the elements
Don’t require extra memory	Require extra memory


@SpringBootApplication - @ComponentScan (used for component scanning), and @EnableAutoConfiguration

Database sharding is the process of storing a large database across multiple machines.
The word “Shard” means “a small part of a whole“. Hence Sharding means dividing a larger part into smaller parts. 
In DBMS, Sharding is a type of DataBase partitioning in which a large database is divided or partitioned into smaller data and different nodes.

Consider a very large database whose sharding has not been done. For example, let’s take a DataBase of a college in which all the student’s records 
(present and past) in the whole college are maintained in a single database. So, it would contain a very very large number of data, say 100, 000 records.
 Now when we need to find a student from this Database, each time around 100, 000 transactions have to be done to find the student, which is very very costly. 
 Now consider the same college students records, divided into smaller data shards based on years. Now each data shard will have around 1000-5000 students records only.

sharding horigental and vertical : 

Horizental : ex 1,2,3,4 rows we have then in one shard 1 and 2 rows and in second shard 3,4 rows
vertical : a,b,c,d cols in one table then in one shard a,b,c cols with data and in second shard a,d cols where a is the primary key.

shard key(hash based and range based)(using hash based it generate table where:  primary key -> shard no) is used to identify the correct shard from multiple shard .


------------------------------------------------------------------------ScheduledExecutorService----------------------------------
executer service and scheduled executer service:
The ScheduledExecutorService accepts both Runnable and Callable tasks.
The scheduleAtFixedRate method will try to execute the task always with the same defined period.

ScheduledExecutorService ses = Executors.newScheduledThreadPool(1);
        Runnable task2 = () -> System.out.println("Running task2...");
        task1();
        //run this task after 5 seconds, nonblock for task3
        ses.schedule(task2, 5, TimeUnit.SECONDS);
        task3();
        ses.shutdown();
		}
		
public static void task1() {
        System.out.println("Running task1...");
    }

    public static void task3() {
        System.out.println("Running task3...");
    }
Running task1...
Running task3...
Running task2... //display after 5 seconds

ScheduledExecutorService ses = Executors.newScheduledThreadPool(1);
        Callable<Integer> task2 = () -> 10;
        task1();
        //run this task after 5 seconds, nonblock for task3, returns a future
        ScheduledFuture<Integer> schedule = ses.schedule(task2, 5, TimeUnit.SECONDS);
        task3();
        // block and get the result
        System.out.println(schedule.get());
        System.out.println("shutdown!");
        ses.shutdown();
		
 public static void task1() {
        System.out.println("Running task1...");
    }

    public static void task3() {
        System.out.println("Running task3...");
    }
Running task1...
Running task3...
10					//display after 5 seconds
shutdown!

---------------------------------------------------------------------------Index--------------------------------
create index:
CREATE INDEX idx_pname
ON Persons (LastName, FirstName);

ALTER TABLE table_name
DROP INDEX index_name;

ALTER TABLE EMPLOYEE (
    ADD INDEX first_name, 
    ADD INDEX department, 
    ADD INDEX joining_date
);

SHOW INDEX FROM table_name FROM db_name;

There are two types of databases indexes:
Clustered
Non-clustered
Both clustered and non-clustered indexes are stored and searched as B-trees, a data structure similar to a binary tree.
Here is a B-tree of the index we created. Our smallest entry is the leftmost entry and our largest is the rightmost entry. 
All queries would start at the top node and work their way down the tree, if the target entry is less than the current node the left path is followed,
 if greater the right path is followed
 
 The clustered index will be automatically created when the primary key is defined:
 
 The indexes other than PRIMARY indexes (clustered indexes) called a non-clustered index. The non-clustered indexes are also known as secondary indexes.
 The non-clustered index and table data are both stored in different places. It is not able to sort (ordering) the table data. 
 The non-clustered indexing is the same as a book where the content is written in one place, and the index is at a different place.
 MySQL allows a table to store one or more than one non-clustered index. 
 The non-clustered indexing improves the performance of the queries which uses keys without assigning primary key.
 
 A table can contain one or more than one non-clustered index.
 
----------------------------------------------------------------------------------------------------------------------------

API Gateway Pattern : Instead of requests directly accessing any microservice, 
                       the API Gateway routes the requests to the appropriate microservice and validates their credentials (authentication)
					   An API Gateway is the single point of entry for any microservice call.

						It can work as a proxy service to route a request to the concerned microservice, abstracting the producer details.
						It can fan out a request to multiple services and aggregate the results to send back to the consumer.
						One-size-fits-all APIs cannot solve all the consumer's requirements; this solution can create a fine-grained API for each specific type of client.
						It can also convert the protocol request (e.g. AMQP) to another protocol (e.g. HTTP) and vice versa so that the producer and consumer can handle it.
						It can also offload the authentication/authorization responsibility of the microservice.

Service Registry and Discovery Pattern: service which enables the microservices to dynamically set and communicate with each other by 
                                        registering and de-registering them as they start and stop	
                                       It stores key details like IP address and port number.	It keeps track of services when they begin or end
Circuit Breaker Pattern:  when repeated service calls fail and redirecting requests to fallback methods or default responses to prevent system failure.
                           For example: In the real world, let an e-commerce application depend on the “Payment Service” for processing transactions where regular 
						   requests are sent. If the system detects delays or errors in the payment service responses, 
						   it triggers the circuit breaker pattern by stopping further requests for the service so, instead of showing more failed requests, 
						   it stops them and shows users a message “Service is temporarily unavailable, please try again later” as we generally receives sometimes.		
Database per Service: data integrity, isolation, and prevents it from direct data access between the services.
Serverless Microservice (FaaS) : FaaS merges two of the latest software development concepts such as microservices and serverless computing,
                               which offer a scalable and cost-effective approachs for the applications. In serverless computing, 
							   the developers write their codes without concern for themselves, leaving the cloud providers to handle the infrastructure and scaling automatically.
                               It merges microservices with serverless computing.							   
							   
CQRS (Command Query Responsibility Segregation) Pattern: It differentiates the read and write operations in a system,
                                                         This pattern splits the responsibility of handling commands that change data,
    														 from handling queries that retrieve data in software systems.		
    Command Responsibility: Write Operations:, 	Validation , Transactional(Commands often execute within transactional boundaries to guarantee atomicity, consistency, isolation, and durability (ACID properties).) 															 
	Query Responsibility  :   Read Operations, Optimized Data Retrieval:(This may involve denormalizing data, employing caching strategies, )


DELETE FROM INFORMATION
WHERE SER_NO NOT IN (
	SELECT MIN(ser_no) as MIN_ID
	FROM INFORMATION
	GROUP BY emp_name, section, contact_info, location
)
-----------------------------------------------------------------Thread----------------------------------------------------------------

New Thread: 	When a new thread is created, it is in the new state. The thread has not yet started to run when the thread is in this state. When a thread lies in the new state, its code is yet to be run and hasn’t started to execute.
Runnable State: A thread that is ready to run is moved to a runnable state. In this state, a thread might actually be running or it might be ready to run at any instant of time. It is the responsibility of the thread scheduler to give the thread, time to run. 
				A multi-threaded program allocates a fixed amount of time to each individual thread. Each and every thread runs for a short while and then pauses and relinquishes the CPU to another thread so that other threads can get a chance to run. When this happens, all such threads that are ready to run, waiting for the CPU and the currently running thread lie in a runnable state.
Blocked: 		The thread will be in blocked state when it is trying to acquire a lock but currently the lock is acquired by the other thread. The thread will move from the blocked state to runnable state when it acquires the lock.
Waiting state:  The thread will be in waiting state when it calls wait() method or join() method. It will move to the runnable state when other thread will notify or that thread will be terminated.
Timed Waiting:    A thread lies in a timed waiting state when it calls a method with a time-out parameter. A thread lies in this state until the timeout is completed or until a notification is received. For example, when a thread calls sleep or a conditional wait, it is moved to a timed waiting state.
Terminated State:  A thread terminates because of either of the following reasons: 
Because it exits normally. This happens when the code of the thread has been entirely executed by the program.
Because there occurred some unusual erroneous event, like a segmentation fault or an unhandled exception.

Q-18 What is the difference between the start() and run() method?
First, both methods are operated in general over the thread. So if we do use threadT1.start() then this method will look for the run() method to create a new thread. 
While in case of theadT1.run() method will be executed just likely the normal method by the “Main” thread without the creation of any new thread.

Note: We can not restart the same thread again as we will get IllegalThreadStateException from java.lang package. 
Alongside we can not do this indirectly with usage of ‘super.start()’ method.

We must call the wait method otherwise it will throw java.lang.IllegalMonitorStateException exception. 
Moreover, we need wait() method for inter-thread communication with notify() and notifyAll().
 Therefore It must be present in the synchronized block for the proper and correct communication.
 
 Under preemptive scheduling, the highest priority task executes until it enters the waiting or dead states or a higher priority task comes into existence. Under time slicing, a task executes for a predefined slice of time and then reenters the pool of ready tasks.
 The scheduler then determines which task should execute next, based on priority and other factors.
 
 10) What is context switching?
In Context switching the state of the process (or thread) is stored so that it can be restored and execution can be resumed from the same point later. 
Context switching enables the multiple processes to share the same CPU.

public final void join() -->> Main --> t1.join() --> so it will put wait to main thread(current) till t1 get executed/ dead then Main thred execute.

Wait() method belongs to Object class.						Sleep() method belongs to Thread class.
Wait() method releases lock during Synchronization.			Sleep() method does not release the lock on object during Synchronization.
Wait() should be called only from Synchronized context.	   There is no need to call sleep() from Synchronized context.

CompletableFuture --> chainng + exception handle
 CompletableFuture.supplyAsync(() -> "Task 1")
            .thenApply(result -> result + " + Task 2")
            .thenApply(result -> result + " + Task 3")
            .thenAccept(System.out::println);
    }
	
CompletableFuture.supplyAsync(() -> {
            if (true) throw new RuntimeException("Something went wrong!");
            return "Success";
        }).exceptionally(ex -> {
            System.out.println("Error: " + ex.getMessage());
            return "Fallback result";
        }).thenAccept(System.out::println);


One of the powerful features of CompletableFuture is its ability to compose multiple asynchronous operations. We can use methods like thenApply, thenCombine, thenCompose 
to perform operations on the result of one CompletableFuture and create a new CompletableFuture as a result.

CompletableFuture<String> helloFuture 
            = CompletableFuture.supplyAsync(() -> "Hello"); 
        CompletableFuture<String> greetingFuture 
            = CompletableFuture.supplyAsync(() -> "World"); 
  
        CompletableFuture<String> combinedFuture 
            = helloFuture.thenCombine( 
                greetingFuture, (m1, m2) -> m1 + " " + m2); 
  
        System.out.println(combinedFuture.get()); 

CountDownLatch is a thread waiting for multiple threads to finish or calling countDown(). When all threads have called countDown(), the awaiting thread continues to execute.	
will throw InterruptedException.

       // Let us create task that is going to
        // wait for four threads before it starts
        CountDownLatch latch = new CountDownLatch(4);
 
        // Creating worker threads
        Worker first = new Worker(1000, latch, "WORKER-1");
        Worker second = new Worker(2000, latch, "WORKER-2");
        Worker third = new Worker(3000, latch, "WORKER-3");
        Worker fourth = new Worker(4000, latch, "WORKER-4");
 
        // Starting above 4 threads
        first.start();
        second.start();
        third.start();
        fourth.start();
 
        // The main task waits for four threads
        latch.await();
		// Main thread has started, this line will wait for all 4
        System.out.println(Thread.currentThread().getName()
                           + " has finished");


---------------------------------------Global Exception----------------------	
@RestCOntrollerAdvice
class extends ResponseEntityExceptionHandler

@ExceptionHandler(DataNotFound.class)
ResponseEntity<String/ErroCode> handleException(DataNotFound data){
 retrun ResponseEntity.badRequest().body(data.getMessage())
}

--------------------------------short URL-------------------
.com/bigURL
Post: -->> getShortURL(bigURL ){
 using MD5/SHA/UUID/Base64 hasing technique we can generate the hash code and return.
}

.com/key(shortURL)
Get: -->> getAndRedirectTOLongURL(key/sorturl)
fidn teh key from cache/DB and redirect to actual/big url.

------------------------Implement Swagger in springboot-------------
gradle: springfox-swagger2 && springfox-swagger-ui   OR  single springfox-boot-starter

import springfox.documentation.spring.web.plugins.Docket;

@Configuration
@EnableSwagger2
public class SpringFoxConfig { 
  public Docket SwaggerApi() {
       return new Docket(DocumentationType.SWAGGER_2)
               .select()
               .apis(RequestHandlerSelectors.any())
               .paths(PathSelectors.any())
               .build();
   }                                   
}

---------------------------OR ---------------
				   
@SpringBootApplication
@EnableSwagger2
public class SwaggerDemoApplication {
   public static void main(String[] args) {
      SpringApplication.run(SwaggerDemoApplication.class, args);
   }
   @Bean
   public Docket productApi() {
      return new Docket(DocumentationType.SWAGGER_2).select()
         .apis(RequestHandlerSelectors.basePackage("com.tutorialspoint.swaggerdemo")).build();
   }
}

----------------------------------------------Restaurant Reservation------------------------------------
Table - 3 --> Customer and Restaurant(table layout(seat per table), address, name , timing) and Reservation (id, cusId, resId, timeslot, party size, Notes(special occasion))

start with nornalize tables later if most join operation happen in future then we can go with denormalizde table means all in one table.(not good to modify when one field modify all table will process)

-----------------------------------------------------------------
FROM openjdk:18                                    -->>    use java 18
WORKDIR /app                                       -->>>>> workign dir inside the container
COPY ./target/spring-0.0.1-SNAPSHOT.jar /app                
EXPOSE 8080                                          the container will listen on port 8080 at runtime  
CMD ["java", "-jar", "spring-0.0.1-SNAPSHOT.jar"]    command to run when the container starts. launch Spring Boot app by executing the java -jar command with JAR as its argument.


$ docker build -t [name:tag] . -->> create a docker image by using the docker build command:
$ docker run -d -p [host_port]:[container_port] --name [container_name] [image_id/image_tag]  -->> Create a docker container by running following command:
	-d = run the container
	-p mapping port for our container.
	-name =assign name to container.

$ docker container ps   -->> Verify whether the container has been created successfully by running below command:

-----------------------------------------------------Kafka ---------------------------------------
Topic -> has multiple partition - Each msg within one partition get the increment id called offset .

once data written in partiion it can not change so topic are immutable .
data kept for one week (default) also configurable .
offset are meaning to a specific partition so offset 3 in partion 0 has different value with offset 3 of partion 1
data assign randamly to partion unless a key is provided.

if prducer send key(number,string,binary etc) then all mesg to that key will got to same partiion if key==null then msg will go to diffrent partionion

Messg contains (by prducer) -->> Key - VALUE
							Compression Type
							Header (optional) with key-value pair
							Partition + offset
							Timestamp(by system or user can set)
							
Kafka msg serializable - kafka accept only bytes as input and send as byte to consumer. this for key value only.
if Key is Integer then keyserlizer = IntegerSerlizer	and valueSrlizer = StringSerlizer	

Kafka partitionor : code logic to determine : record to send which partition.
key hashing - is the proess to mappoing of key to partition. its by murmur2 hasing algo - msg will go to whihc partition.		

COnsumer read data from partiion from low to high offset: 1,2,3 But no order gurranty in acrocc partition 1 and 2.
DOnsumer Deserlizer

__consumer_Offsets : while reading kafka write into consumer_offset so thaat if consumer die then when it come back it will start from there only.

kafka broker / server / bootstrap server 		 : each broker knows about all other broker and topics so if client connect to one broker then it get get other broer too.
At any time only one broker can be leader for a given partition.
producer can send data only to the broker  that is leader of  a partition.	consumer is do the same get from the leader only.

ask=0 : producer will not wait for ack from broker, data written or not (possibe data lose if broker down)
asks=1 : producer will wait for ack
asks=all : Leader + replicas will ack : no data lose

Producer:   ----
//Create producer properties
  properties.setProperties("bootstrat.server","127.0.0.1:1980")  -- for localhost.
  "key.serliazer",""
  "value.serliazer",StringSerlozer.class.getName()
  "security.protocol",StringSerlozer.class.getName()
  username , pass 
  
// create producer
   KafkaProducer<String,String> pro = new KafkaProducer(properties)

// Create producer Records:
    ProducerRecord<String,String pr = new ProducerRecord("topicName","value") OR ProducerRecords(topic,key,value) : if we want eo send key

//send data
    pro.send(pr)   or we can use new callback() with prudcerRecord: this will override onCOmplete method which call on every seccess send and error/exception
// close producer
    pro.flush
	pro.close()	


Sticky partition: send data in batch do performance increase.


// Consumer:  deserlizer and group.id inside  properties.  + auto.offset.reset,"none/earlist/latest"  none:set consumer grop on stating
   earlist: read from the start. latest: read from the latest/new.
   
   
kafkaConsumer k = new KafkaConsmer(proeletiees)

k.subscribe(topic)
ConsumerRecodes  cr = k.poll(time in ml )  // time : wait for sme second if mess is not there.

get data from cr.
------------------------------------------rate limiter--------------------------------------------------------

A rate limiter prevents DoS attacks,
Reduces cost where the system is using a 3rd-party API service and is charged on a per-call-basis.
To reduce server load

The client is an unreliable place to enforce rate limiting because client requests can easily be forged by malicious actors.

Token Bucket Algorithm : This algorithm has a centralized bucket host where you take tokens on each request, If the bucket is empty, reject the request.
   In our case, every Stripe user has a bucket, and every time they make a request we remove a token from that bucket. We implement our rate limiters using Redis.
   The bucket has tokens with a pre-defined capacity. When a request comes in it takes a token from the bucket and is processed further. If there is no token to be picked, 
   the request is dropped and the user will have to retry.

The HTTP 429:::::: Too Many Requests response status code indicates the user has sent too many requests in a given amount of time.

Example use-case:
We have our rate-limiting rules set to 3 requests per user per minute.
User1 makes the request at 00 time interval, currently the bucket is full with 3 tokens so request will be processed. Tokens will be updated in the bucket to 2 now.
User1 makes a 2nd request at 10th sec, bucket has 2 tokens so again the request will be processed further. Tokens will be updated in the bucket to 1 now.
User1 makes a 3rd request at say 30th sec, bucket has 1 token so again the request will be processed further. Now the bucket is empty till the whole 1 minute completes.
User1 makes a 4th request at say 55th sec, the bucket has 0 tokens so the request is throttled and the user is returned with a 429 status code — too many requests 
     and asked to retry it later in some time. The HTTP 429 Too Many Requests response status code indicates the user has sent too many requests in a given amount of time.
At the completion of that 1 minute, the token count is refreshed at a fixed rate and the bucket is again full with 3 tokens and requests can be processed again for that 
  particular user.

Rate Limiter Middleware fetches rules from the cache. It also fetches data such as timestamps, counter etc. from Redis. 
Then it does the computation when a request comes and decides whether to process the request or rate limit it.

 Redis Lua Scripts. : resolve race condition or syncronization.
 
  --------------------------Image vs BLOB for image save----------------------------
 
DB/BLOB provide : Transaction + Index + Immutability - if we dont need these in case of image save we can go with File 
  (cheaper and faster(Selsct * will give all the big data if we use BLOB).)
DB table : Id name and path(path of file.)

------------------------------------------WebClient---------------------------------------------------------------------
An interface called WebClient represents the primary access point for web requests. IT is introduced in Spring 5. 
it is the replacement of classic RestTemplate.
 We can use the WebClient for Java Microservices Communication by the following approach.
 <artifactId>spring-boot-starter-webflux</artifactId>
 
 @Bean
public WebClient webClient() {
  return WebClient.builder().baseUrl(addressBaseUrl).build();
}

@Service
public class EmployeeService {
    @Autowired
    private WebClient webClient;	
	
        AddressResponse addressResponse = webClient.get().uri("/address/" + id).retrieve().bodyToMono(AddressResponse.class).block();
        employeeResponse.setAddressResponse(addressResponse);
		
}
  --------------------------------------------------Rate limiter in API Gateway-------------------------------------------------------------------------------------------------------
rate limiter by its own 
   predicates:
            - Path=/backend
          filters:
          - name: RequestRateLimiter
            args:
              rate-limiter: "#{customRateLimiter}"
              key-resolver: "#{customKeyResolver}"

rate limiter using redis: 
    predicates:
            - Path=/backend
          filters:
          - name: RequestRateLimiter
            args:
              redis-rate-limiter.replenishRate: 500
              redis-rate-limiter.burstCapacity: 1000
              redis-rate-limiter.requestedTokens: 1

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Security in Spring Boot microservices can be ensured by implementing features like authentication and authorization using Spring Security. 

Centralized configuration management::::: allows for dynamic configuration changes across microservices without redeploying them. 
Tools like Spring Cloud Config Server enable the storage of configurations in a central repository and their distribution to microservices as needed.
This includes techniques such as OAuth2 for token-based authentication and role-based access control.

How would you design resilient microservices in Spring Boot to handle failures gracefully?
	Resilient microservices in Spring Boot can be designed by implementing patterns such as circuit breakers (using libraries like Resilience4j or 
	Netflix Hystrix), retries, timeouts, and fallback mechanisms to gracefully handle failures and prevent cascading failures across the system.

Eureka is a service registry and discovery server provided by Netflix. Its role in microservices architecture is to help services register
 themselves and discover other services dynamically, enabling communication between them.
 
Circuit breaking is a design pattern used to handle and prevent cascading failures in microservice architectures. 
 It involves monitoring the calls to external services and, if a certain threshold of failures is reached, tripping a circuit breaker to stop further 
 requests and return a fallback response. 
 In Spring Boot, circuit breaking can be implemented using libraries like Netflix Hystrix
 
ensure data consistency across multiple microservices?
 Using the Saga pattern: 
 Using event-driven architecture: Services can communicate through events, and changes in one service can be propagated asynchronously 
   to other services, ensuring eventual consistency. 
   
What is Spring Cloud Bus?
 This module contains a lightweight message broker implementation, which is mainly used to broadcast some messages to the other services

What is Spring Cloud Stream?
	This module mainly allows us to implement asynchronous communication between our microservices using event-driven architecture. 
	We can use Apache Kafka or RabbitMQ as a message broker to implement event-driven microservices.

--------------------------------------------Optional-------------------------------------------------	
@GetMapping("/api/foos")
@ResponseBody
public String getFoos(@RequestParam(required = false) String id) { 
    return "ID: " + id;
}

http://localhost:8080/spring-mvc-basics/api/foos?id=abc    
ID: abc
http://localhost:8080/spring-mvc-basics/api/foos
----
ID: null

Using Java 8 Optional ::::::::::::::::::::;;
@GetMapping("/api/foos")
@ResponseBody
public String getFoos(@RequestParam Optional<String> id){
    return "ID: " + id.orElseGet(() -> "not provided");
}
-------------------------------------------default value---------------------------------------------------------------
@GetMapping("/api/foos")
@ResponseBody
public String getFoos(@RequestParam(defaultValue = "test") String id) {
    return "ID: " + id;
}

A single @RequestParam can have multiple values:

@GetMapping("/api/foos")
@ResponseBody
public String getFoos(@RequestParam List<String> id) {
    return "IDs are " + id;
}
http://localhost:8080/spring-mvc-basics/api/foos?id=1,2,3
----
IDs are [1,2,3]

-----------------------------------------------Challange--------------------------------------------------------------
if error in dependent service then it hard to find so we use the proper corellation id so can know where is the error.

------------------------------------------------------------------------------------------------------------------------
Cohesion refers to the degree to which elements within a module work together to fulfill a single, well-defined purpose. Increasing cohesion is good for software.
	Cohesion represents the relationship within a module.

Coupling represents the relationships between modules. Increasing coupling is avoided for software
	Coupling represents the relationships between modules.
	
----------------------------------------------------------Eureka Server -------------------------------------------
<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
eureka.client.register-with-eureka=false   																-->> not register with other eureja instant
eureka.client.fetch-registry=false         																-->> no tfetch the registry from ther eureka server.
eureka.instance.client.serviceUrl.defaultZone=http://localhost:8761/eureka/

When a service is registered with Eureka Server it keeps sending heartbeats for certain interval. If Eureka server didn't receive a heartbeat from any service instance
 it will assume service instance is down and take it out of the pool.

-------------------------------Client------------------------------
<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>

eureka.client.service-url.defaultZone=http://localhost:8761/eureka/

Note: Why we need registry and discovery : supoose we have 2 service 1 and 2 service-2 have two instances (1,2 with diffrent port), so if we want to call the rest point ofservice 
-2 from service-1, then we use LoadBalancer with configure poth instances with diffrent port, but if we want to increase/ decrease the instance od service-2 then we need to
update the loadBalancer cond=fig in service-1 too, SO to overcome this overhead we use this where instanc will register automatically with service-registry.

The RestTemplate with @LoadBalanced annotation will internally use Ribbon LoadBalancer to resolve the ServiceID and invoke REST endpoint using one of the available servers.
@Bean
    @LoadBalanced
    public RestTemplate restTemplate() {
        return new RestTemplate();
    }

Note that we have used http://inventory-service/api/inventory/{code} instead of http://localhost:9898/api/inventory/{code} or http://localhost:9999/api/inventory/{code} directly.

-------------------------------------------------------------------------------------------------------

@HystricCommand(fallbackMethod=”defaultProductList”, commandProperties = { @HystrixProperty(name=”execution.isolation.thread.timeoutInMilliSeconds”, value=”500”)})

 To enable the Hystrix dashboard you have to add HysrixDashboard dependency in the classpath and you have to replace the @EnableCircuitBreaker annotation with two annotation
 @EnableHystrix and @EnableHysrixDashBoard.
 
--------------------------------------------------Session vs JWt ----------------------------------------
Session based auth: server resposible for creating and stroingthe session (create sesion and store on DB/Redis and send back session id to clinet )

JWT based : validate the first request and create jWT token with sign algo and send back to client with cookie and client send this jwt token with subsequence request.
    In this no seperate storage needed.
	invalidate o JWT is not easy

-------------------------------------------Auth2----------------------------
user --->>>>(/home) client(our outh client/controller where client/secret id in app.yml (generated in google/auth server)) ---Security filter chain-->> 
redirect to google auth server (with client-id, redirest url,
scope, res type)  ----->> googel send the auth code to client --->>> then security filter set this auth code to Authentication object then AuthManager send 
this object to AuthenticationProvider
AuthProvide has diffrent type for oth2 OAuth2AuthCodeAuthentication Provider is there. then OAuth2AuthCodeAuthentication Provider call the google auth server 
with auth code, client-id,
client-secret, redirect URL , based on these details google retrun the Token to OAuth2AuthCodeAuthenticationProvider, again OAuth2AuthCodeAuthentication privider call goole with 
Token to get user details and google return the user details to OAuth2AuthCodeAuthenticationProvider. then OAuth2AuthCodeAuthentication set these details to 
Authentication object and sent to 
AuthManager -->> AuthenticationManager send back to SecurityFilterChain -->> SecurityFilterChain set the auth obj to Security COntext before allowing to user to /home page.
One More -->> after seting auth obj to Security Context It will create the one session obj inside this it will store securityCOntext with Auth object. and send this session id 
to user.
sesson id set into browser.

Second request SecurityFilter get the session 7id and check is these any sesion availabel if yes then allow /home 

redirest url :localhost:8080/login/oauth2/code/google  this URL is code by dependency

Get user info from Authentication object that is set by OAuth2AuthCodeAuthenticationPro to Auth Manager and set to Security COntext.
Authentication a = SecurityContext.getContext().getAuthentication()
OAuth2User user = a.getPrinciple()
Map<String,Object> map = user.getAttributes()

OAUTH 2.0 : If you dont want to share the user/pass to third party/site to access the third party site, then use the username/pass of google/facebook to use where you already registered.
   so it grand authority to some other apps(in this case client where we dont want login/give password to the client) so they can act on your behafe.
   
   Client: Where we hit /home page or Where we dont want to register or dont want to give pass
   Resource owner: Me.
   Resource Server  : where my data store.
   Auth Server: Goolge/Facebook.
   
-----------------------------------OpenId Connect---------------------------------------------
Client/APP1 call with client-id and redirect-url + one extra(Scope: OPENID) ---->>>> Authorization Server (which contains OPENIDConnect-Provider and UserInfo Endpoint)
Auth Server return same Authorization code/Access Token + ID-Token (It contains Claims like Subject,Name etc it is small), if Client app want more info about User then -
   it use the Access Token and connect to the UserINfoEndpoint inside Auth Server.

Way to implement OIDC 
UserInfo Endpoint 
ID-Token 

---------------------------------------------------Liscov---------------------------------------
Parent class subtype can be replace with each other or with parent without replace/modify the existing code.

Class Vehicle: 
 getWheeel(){
  return 2 }
  
 hasEngine(){
 true }
 
Class MOtercycle extends Vehicle{
 // nothing to override. bcoz it has engine + 2 wheel.
 }
 
 Class Car extends Vehicle{
  override
  getWheel(){
  return 4;
  }
 }
 List<Vehile> li = new ArrayList()
 li.add(newMoterCycle())
 li.add(newCar())
 Iterate with for loop and print values + toString()
 
 //Problem  : if we add one more class cycle whihc extends to Vehicle in this case it doest have engine and it can be return null exception.
 We we add one more
 Class cycle extends Vehicle{
  override
  hasEngine(){
    null }
 }
 
 // SOlution : add only Genric method to Vehicle Class : getWheel(){
 return 2
 }
 Now Cycle extends to Vehicle 
 
 Add create another class EngineVehicle with hasEngineMethod() extends Vehicle -->> and Motorcycle and Car class extends EngineVehicle
 
 
 -----------------------------------------------------------------------------------
ACID: 
  Atomicity: all or nothing : either all operation performed within tran get execute or nothing. menas if commint the tran then it perform all the operation or else rollback.
  Consistency: if data update in one system then it should reflect in other system too like in bank money cut from one account then should deposite in other account 
  Isolaton: canges in one tran should not visible to other tran util it commit them successfully.
  Durability: commit changes hould be persist.
  
----------------------------------------------------------------API Gateway-------------------
add dependecy 
spring-cloud-gatway + Eureka-client dependecy ,(register into discovery server)

Also add
@EnableEurekaClient

eureka.client.serviceUrl.defaultZOne = http://localhost:8761/eureka   // reggister in discovery server.
spring.application.name = APIGATWAY
route 1-
spring.cloud.gateway.routes[0].id = PRODUCT-SERVICE
spring.cloud.gateway.routes[0].uri = http://PRODUCT-SERVICE / lb://PRODUCT-SERVICE  -- lb: auto client side load-balancing by api gateway
spring.cloud.gateway.routes[0].predicates[0] = Path=/api/product

route  2-
spring.cloud.gateway.routes[1].id = ORDER-SERVICE
spring.cloud.gateway.routes[1].uri = http://PRODUCT-SERVICE / lb://PRODUCT-SERVICE  -- lb: auto client side load-balancing by api gateway
spring.cloud.gateway.routes[1].predicates[0] = Path=/api/product

NOTE: server.port = 0 // means server will start on available port which is availabl on machine
  
spring.security.oauth2.resourceserver.jwt.issuer-uri= issue-url get from  Auth server.  inside : openid config file

@Configuration
@EnableWebFluxSecurity // bcoz api-gatway is work on top of Spring WeFlux not on spring WebMVC.
Class SecurityConfig{

	@Bean
	public SecurityWebFilterChain springFIlter(ServerHttpSecurity http){
		http.csrf().disable()
		.permitAll()
		.oauth2REsourceServer(ServerHttpSecurity.OAuthResourceServerSpec::jwt);
	return http.build();
	
	}
	


}
After that we need to create the Access-token using postmant etc. by giving details:

client-id
clent secret: 
access-token uri
scope: openid

These detail are present in auth server config file
-----------------------------------------------------------------------------------
Hystric OR resiliance4J  :
	CLOSE (When no outage or service is avalialbel )  -->>   OPEN (When service is down and fallback method is calling)      -->> HalfOPEN (checking actual service is up or not)
	 properties: 
	1 - no of call to service if it is down:
	2 - time till circuit breaker is open. or fallback method will be called.
	3 - no of call to service to check,if it is up or not from HalfOPEN stage.
	
----------------------------------------------------------AOP-------------------------------
It enable the crosscutting concern (loggig/trans mang which is not part of business logic) 
		
		Class A ---->>>> Class B 
		                  //business Logical
						  // loggig  -   cross cutting concern 
						  

                Class AOP/Asspect( 
				// loggig() -- Advice    
					^         |
					|         |
					|         ->
		Class A ---->       	Class B 
								//business Logic()  -- Joint point
						  
						  
Aspect:      class or concern which cut from the business logic. ex log, trans manag ----->>> Implement by XML or @AspectJ style.
Joint Point:  Class B business logicn method is the joint point. before invocation of this method and after invocation of this method we call our Aspect
Advice:    action taken by the the aspect here logging()
pointcut: This aspect will be applied on whihc method. these details are part of point cut/point cur exr=oression.

AOP Proxy : Spring AOP is proxy based.

		class A ------->>>>>  proxy -->> class B 
		
		SO when class A call the Class B  method, first it will land to Proxy and proxy check, any Aspect is there for this class or not if yes, then it call the aspect/advice,
		once advice finished then call the Class B actual method , based on point cut expression.
						  
	Two type of Proxy: 
				1 - JDK dynamic proxies: when interface implemented
				2 - CGLIB  : if business obj does not imp interface means we do not have interface we have class.
				
@EnableAspectJAutoProxy : it will auto come if we have @SprinBootApplication

Aspect : 
    @Component
	@Aspect
	public class loggAspect(){
	
		@Before("execution(* method path.addProduct(..) ))"   // * is return type(here anything), .. is parameter can be anything.
		public void log(){
		
		}
		
	}	
	@Before: before the execution of addProduct() it call the log()    ----->>>> point cut expression.
	@After : after call.
	
	    @Around("execution(* method path.addProduct(..) ))"   // * is return type(here anything), .. is parameter can be anything.
		public Object log(ProceedingJointPoint jointPoint){
			sop("before joint point method or business")
			Object res = jointPoint.proceeed();
			sop("after business call")		
		}
	
	@AfterReturning("execution(* com....(..))")  -->> after business methos run normally if you want to run some step then use it.
	@AfterThrowing: -->>>>>> if you want to throw exceotion after business n=methos executionl
	
---------------------------------OR -------NamedPointCut---------------	
	@PointCut("execution(* method path.addProduct(..) ))" 
	private void pontCUtMe(){                                use this pontCUtMe() whereever we need.
	}
	
	   @Around("pontCUtMe")                  
		public Object log(ProceedingJointPoint jointPoint){
			sop("before joint point method or business")
			Object res = jointPoint.proceeed();
			sop("after business call")		
		}
---------------------------------------------------------------	PointCut Designator----------------------------
execution:
within: 
	@PointCut("within(* classPath to ProductService ))"     --  >> all the method within this call will be intercepted by this PointCut expression.
@within:   -->> for class annotation
	@PointCut("@within(* org.springwork.sterotype.Service ))"     --  >> all the class which annotted with @Service, those class all methods will be intercepted by this PointCut expression.
	
@annotation:   -->> for method/function annotation.
	@PointCut("@annotation(* org.springwork.web.bind.annotaion.PostMapping ))"     --  >> all function with annotation Postmapping will be intercepted by this PointCut expression.
	
	
target:   -->> 
	@PointCut("target(* com...ProductService ))"     --  >> when this productService will init and call any method before that method our Adi=vice/Aspect will call by expression.
															supose we have controller and we have ProductService init and getting use in multiple methods to call it so it will apply to all methods
	
@target:   -->> work on annotation
	@PointCut("target(* org.springwork.sterotype.Service ))"     --  >> all the call whihc are annoted with @Service and if those class will call any method, before those methods
	                                                                 that Aspect will called.
																	 
@ : work with annotation and other work with Classes.

we can also combine the pointcut expression:
	@PointCut("within(* classPath to ProductService ) && @within(* org.springwork.sterotype.Service ))"  
	    -->> so all the methods within ProductService and all the method within class that is annotatted with @Service will intercept the Aspect. once both true then only intercept.
		
---------------------------------------------------Transaction---------------------------------------------------------------
@EnableTransactionManagement    -- auto config so not needed explicitly
need data-jpa dependecy for @Transactional

TransctionManagement in Spring usees AOP:
	@within("org.springframework.annotaton.Transaction") 

// It use above pointcut expression to serach the methods having @Transactional oce match it run the Around type Advice.
	invokeWithinTransaction() present in TransactionInterceptor class.
	
	class TransactionInterceptor/TransactionAspectSupport(){
	
	    @Around("@within("org.springframework.annotaton.Transaction") ") 
		invokeWithinTransaction(){
		//create Transaction 
		//invocation.proceedWIthInvocation
		//commit/rollback transaction.
		}
	
	}

Propogation:
Required: default it use the existing trans or if not create the new trans
Required_New: create new trans every time.
Mandatory: It need the trans from upper side, wont create new trans so throw error if not get trans. SO it force to have trans to run method with mandatory.
NEVER: It throw error if it get any trans 
SUPPORT: It use the existing but wont creat the new trans like REquired.
NOT_SUPPORT: If we have a seies of method with trans but we want to skip any method from these trans we use this. This will suspend the existing trans and resume 
              after method(not-supported) finished
NESTED: if we want partial transaction we use this. IF existing trans is not these it behave like required and create new. and if existing trans is there then
         join the same trans but it create a save point till first/upper trans and  if this(NESTED) method fail it roll back this only and commit till save point/first trans.
		 
-----------------------------------------------------------Logging----------------------------------------------------------------------------------------------------------------------
SpringBoot default logging LOgBack framwork use. and sl4j(Simple logging facade 4  java) is the implementation of LogBack:
  private(bound to this class) final(no modificaton) static(no obj) Logger log = LoggerFactory.getLogger(PService.class)
  log.info()
                                     OR
	@Sl4j  // comig from lombok	
	class Product(){
	 log.info();
	}
  
another we can use 1- Logs4j from apache. &  2 - java.util.Looging

LEVEL : 
	FATAL & ERROR -->> Sevierty : high
	WARN          -->> MEDIUM
	INFO ,          -->> NORMAL
	DEBUG      -- NORMAL    -->> but this  not by default enable we need to enable explicitly.
    TRACE       -- NORMAL        but this not by default enable we need to enable explicitly.
	
	So to enable this we need to write in application.yml: 
	logging.level.root=warn    -------------->>>>>>>>>>>>>>>>>>>>  So we enable to warn so sonow only WARN and ERROR will be come.
	logging.level.root=INFO										Similer if we write INFO then ERROR,WARN,INFO will come
	logging.level.root=DEBUG									Similer if we write DEBUG then ERROR,WARN,INFO,DEBUG will come
	logging.level.root=TRACE									Similer if we write DEBUG then ERROR,WARN,INFO,DEBUG,TRACE will come
	
	Note: for DEBUG < TRACE when we enable these we get lot of logs so fix this, if we want these all log for the perticulaer package/directory only then:
    	replace root with class package name.
	logging.level.com.bnsf.Service=TRACE/DEBUG 
	
-------------------------------------------------------COmponent Scan----------------------------------------
How app find pojo to convert into the beans or not?

Note: to find how many beans are in app 1 -: ConfigurableApplicationContext context = SpringApplication.run(Application.class, arg);
														context.getBean("nameofBean"); // return the bean.
										2 - useing Spring actuator starter dependency and just call URL localhost:8080/actuator/beans and it will give all the beans in app.

@SpringBootApplication = @SpringBootConfiguration + @EnableAutoConfiguration + @COmponentScan

Note: @Serive / @Compoment: this means this call will convert to Spring component.

@ConponentScan(basePackages = "com.bnsf.","...", "...")   // To add Classes to scan by app which is in other packages:
@ConponentScan use with @Configuraton which is already in @SpringBootApplication

// If we want to exclude 
@ConponentScan(basePackages = "com.bnsf.","...", "...", excludeFilters = @ComponentScan.FIlter(type = FilterType.ASSIGNABLE_TYPE, classes = {PService.class}))

----------------------------------------------@Qualifier and @Primary----------------------------------------------
one interface implemented by more than 2 class then bean issue comes.
@Qualifier has more power than Primary , if Qualifier is there in any class then that will become initialize it will ignore the @primary class to initilize
	if we dont have any Quaifier the if we have Primary in any implemented class that class will initilize.
	
--------------------------------------------------Actuator --------------------------------------------------------------
provide Production ready features : to moniter and manage your application when you puch app to production/dev etc
 It provide some URL/endpoint to do that like health etc.
 
 dependency: spring-boot-starter-actuator 
 
 URL : localhost://actuator    -->> show list of available endpoint   by default only one endpoint will be enable.
 To enable all the endpoint: 
   in application.pro:
          management.endpoint.web.exposure.include=*    // * will enable all we can restrict to limited, after changes/give name in property file.
 To disable one of them:
        management.endpoint.info.enable = false
		
-----------------------------------------------Bean Application Context------------------------------------------------------------------------------
pojo manage by spring is called Bean
IOC container: bean creating,Injection manage deletion manage by this. all bean store here.
Application Context is the implementation of IOC container 

confuration/annotation Metadata ------->>> 
										    ApplicationContext    ------>>>> Ready to use as Bean.
POJO				           ------->>> 

Ways to create bean-
1 - @Componenet --> convert POJO to Bean.
2 - @Configuratin + @Bean

--------------------------------------------------Bean Lifecycle--------------------------------------------------------------------------
Custome Action while Bean Creation/Deletion OR customizing the nature of Bean LifeCycle -->>  if we want to perform action after creation of Bean or before deletion of Bean.

-->> First IOC container will start and search the all the component useing componentScan and create bean like ProductService if this Seric=vice has any dependency like repo,
	OrderService,then it will inject he dependency on it and create the bean.
	
After Bean created if you want to perform any: use implements InitializingBean and override method:   afterPropertySet()   OR @PostCOntruct
Before Bean Delete if you want to perform any: use implements DisposableBean and override method:   destroy()              OR @PreDestroy
         use appCOntext.close() to use in real time from Applicaton main class.
		 
IOC started ->> COntruce Bean/class constructor run ->> Inject dependency into Bean/run injected class constructor ->> @PostConstruct of injected bean then parent class @PostConstruct -->> Use Bean -->> @PreDestroy -->> Bean Destroy
	 |
     |
Configuration loaded

--------------------------------------------------Bean Scope--------------------------------------------------------------------------
@Scope("prototype")

Singleton: Eager loading
		Singel bean instance. default is SIngeltone.	

		class Employee                                   Class User 
		{												{
			@Autowired										User(){
			User user	//2									        //3
															}
			Employee(){									    @PostConstruct
						//1									init(){
			}  												//	      //4
			@postConstruct                                     }
			init(){
			//       //5
			}
	
Prototype:   lazy init : when required then init/created/call-constructor -->> new object will created on each time.
			 if above User is prototype then on every request from conr=troller new object will created and if controller is Prototype then on user/postman call only COntroller
             constructor will call not at run time.
			 it create new instance every time wnen gerBean() is invocked on AppCOntext.
			 
			 SO in Request only one instance created if twice called getBean() on AppContext.

Request: One Bean per HTTP Request	+ Lazy

		Note: We can inject Protype Bean into Singleton scope bean but We can not inject request scope bean in Singleton scope class 	 
			  to fix this we can user request bean with ProxyMode
			  
Session: One Bean per Session(request1, req2,.....)

----------------------------------------------@ConditionalOnProperty-------------------------------------------------------------------
When you dont need been at the time of IOC initialization.(app contex will be bombard)

App A (this need MySQL)    ---->>>> common(mySQL & NO SQL config) <<<<<-------- App B (this need No SQL)

@ConditionalOnProperty(prefix = "noSql", value="enabled", havingValue=false, matchIfMissing = false)
                       prefix + value = Key in property2.	-->> noSql.enabled = true
						havingValue = value                 -->> when havingValue match with property value then only bean created.
						matchIfMissing                      -->> if property does not have these prefix then create bean or not.(true create, false not create.)
						
						
	
----------------------------------------------------------Spring Batch---------------------------------------------------------------

Job Launcher  ---->>   JOB ---->>>> Step ---->>>> 1- Item Reader  2 - Item Processor  3- Item Writer

@EnableBatchProcessing  // no required in Batch5.0/spring3.0

Note: if batch job run onstartup: spring.batch.job.enable = false

@Autowired
JobLauncher jobLauncher;  // interface.

@Autowired
Job job;  

@PostMapping
public String jboLaunch(){
	// launch the job
	JobExecution exe = jobLauncher.run(job,jobParameter)
	
	return exe..getStatus().toSting(); // COMPLETED/NOTCOMPETED

}

@Configuration
class(){
	@Bean
	public Job job(jobRepo, Step step){
	 return new JobBuilder("name", jobRepo).start(step).build();
	}
	
	@Bean
	public Step step(jobRepo, PlatformTransactionManager tran){
	 return new StepBuilder("name", jobRepo)
	 .<Pojo,Pojo>chunk
	 .reader(reader())              //defaine read function, where to read if csv then path of that etc....
	 .processors(processor())      //defaine process function like before save make any changes in pojo like capital letter the name.
	 .writer(writer())             // define the write function. save date in DB.
	 .build();
	}
	
	writer(){
	RepositerItemWriter writer = new RepositerItemWriter()
	write.setRepository(pojo repo)
	}

}

-----------------------------------------------------Kafka interview----------------------------------

One Cluster has multiple Broker and in each broker multiple Topics.

SO recomend atleast 3 Broker so that one will be leader and 2 will be follower where relication can happen.
 So one Leader Broker where Producer write the data and some folower where repicate the data.
 
offset is the unique id given to each message in partition.

Producer can give : topic and Pr=artition but offset managemint by the kafka server only.

Consumer do 2 thing 1- ack on each offset/msg read 2- commit the offset till read so that if ti die and come back no need to read from start

-------------------------------------------------------------------Saga--------------------------------------------------------------------------------------------
each microservicehas its own db and it can manager by local tran mang.
so saga pattern group these local tran and invoke sequencly one by one,
each local trans update dba and public event to the next local trans/service.
if one step fail than daga pattern publish revert trans or rolback to the previois microservice.

Type: 
	choreography:  one event msg broker is there. 
	            adv :    simple, no need of coordination logic.
	            disadv: cyclic dependency bcoz here they consume each other commands. and difficult to track whihc saga participent listen to which commands. + intergation test diffcult.
	orchestration: centralize controller is here which exeute saga req + store and interprets the state of each task. + if service fail revert logic also here.
	               call services using HTTP request.
	        adv: no cyclic dependency. easy to track.
			dis: write controller logic and all + this is the single point if fail then fail the whole workflow.
			
---------------------------------------------------
The array is a fixed-size data structure while ArrayList is not. One need not mention the size of the ArrayList while creating its object. 
An array can contain both primitive data types as well as objects of a class depending on the definition of the array. However, ArrayList only supports object entries, 
	not the primitive data types. 	

Array: It is faster as above we see it of fixed size
List: It is relatively slower because of its dynamic nature 


----------------------------------------------------------
When we call update() method, if the same object is not exist in session cache then the update() method will update the object.
When we call update() method, if the same object is already exist in session cache then the update() method throws an exception called “NonUniqueObjectException”.

Similar to update() method, merge() method also changes the state of detached state to persistent state.
When we call merge() method, It first checks the same object exist in cache
If exist then it will update the cache with the changes, else if object is not exist in cache then it will load the values to cache.	

Update: It used if the session does  not contains an already persistence object/state with the same id, means update workd only within session. ()Non unique objeXce)
Merge: it should be used if we dont know the state of the session. means can make mof=dification any time .

-------------------------------------
load : lazy inti  + throw ex ObjNotFound + proxy(run query when use.)
get: early load   + return null if not found.

------------------------
save: retrun serialiable id
/Persist: void.

---------------------------------------------------create WAR file of springboot app---------------------------------------------------
1 - in main class -->> extends SpringBootServletInitializer and override configure()
2 - <packaging>war</packaging>

optional--
3 - <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-tomcat</artifactId>
        <scope>provided</scope>                               ----->>>>> it will not attached dependecy to war so war will be less size.
    </dependency>
	
marking the embedded servlet container dependency as provided produces an executable war file with the provided dependencies packaged in a lib directory.
This means that, in addition to being deployable to a servlet container, you can also run your application by using java -jar on the command line.
	
<build>
    <!-- Our final war file name is geeks-web-services -->
    <finalName>geeks-web-services</finalName>                 
</build>

                                    OR
File-->> Export -->> war 									
---------------------------------------------------------------------------------
GenerationType.IDENTITY: This strategy will help us to generate the primary key value by the database itself using the auto-increment column option. 
It relies on the database’s native support for generating unique values. 

GenerationType.SEQUENCE: This generation-type strategy uses a database sequence to generate primary key values. 
It requires the usage of database sequence objects, which varies depending on the database which is being used.

GenerationType.AUTO: This is a default strategy and the persistence provider which automatically selects an appropriate generation strategy based on the database usage.

-----------------------------------------------------------------------------------------
int[] a = { 2, -1, 4, 3};
Arrays.sort(a, 1, 4);       -->> [2, -1, 3, 4]

Arrays.sort(a, Collections.reverseOrder());
------------------------------------------------------swap without 3rd variable-----------------
	a = a + b;
	b = a - b;
	a = a - b;
----------------------------------------------------------------------

 input.toLowerCase().matches(".*[aeiou].*");
 
 -----------------------------------------------Prime number------------------------0and 1 are not prime no---------------------------------- 
 public static boolean isPrime(int n) {
		if (n <= 1) {
			return false;
		}
		if (n == 2) {
			return true;
		}
		for (int i = 2; i <= n / 2; i++) {
			if (n % i == 0) {
				return false;
			}
		}

		return true;
	}
	
	
public class PrimeFinder {
public static void main(String[] args) {
 List<Integer> numbers = Arrays.asList(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 17, 19, 20);
	List<Integer> primes = numbers.stream()
	.filter(PrimeFinder::isPrime)
	.collect(Collectors.toList());

	System.out.println("Prime numbers: " + primes);
}
// Method to check if a number is prime
public static boolean isPrime(int number) {
 if (number <= 1) return false;
   return java.util.stream.IntStream.rangeClosed(2, (int)Math.sqrt(number))
 .noneMatch(n -> number % n == 0);
}
}

north express


--------------------------------------------------------fibonacci-------------------------------------------------------
F(N) = F(N-1) + F(N-2)  -->> Reccursion formula
    public static int fibonacci(int n) {
		if (n <= 1)
			return n;

		return fibonacci(n - 1) + fibonacci(n - 2);
	}	
//calling -->>       
   for (int i = 0; i < 10; i++) {
		System.out.print(fibonacci(i) + " ");
	}	
	
						OR
    public static void printFibonacciSequence(int count) {
		int a = 0;
		int b = 1;
		int c = 1;
		for (int i = 1; i <= count; i++) {
			System.out.print(a + ", ");
            a = b;
			b = c;
			c = a + b;
		}
	}	
	
	
int count = 10; // Number of Fibonacci numbers to print

 int[] fib = new int[]{0, 1};

 IntStream.range(0, count).forEach(i -> {
	 System.out.print(fib[0] + " ");
	 int next = fib[0] + fib[1];
	 fib[0] = fib[1];
	 fib[1] = next;
 });
 }


-------------------------------------------------------Odd Number-----------------------------------
public static boolean onlyOddNumbers(List<Integer> list) {
	return list
			.parallelStream() // parallel stream for faster processing
			.anyMatch(x -> x % 2 != 0); // return as soon as any elements match the condition
}

----------------------------------------factorial-----------------------------------
 static int factorial(int n)
    {
        int factorial = 1;
        for (int i = 2; i <= n; i++) {
            factorial = factorial * i;                    -->> factorial = factorial.multiply(BigInteger.valueOf(i));
        }
        System.out.println(res);
        return factorial;
    }
	
	            OR
				
 static int factorial(int n)
    {
        if (n == 0) {
            return 1;
        }
        return n * factorial(n - 1);
    }
----------------------------------------------	linklist reverse ---------------------------
LinkedList<Integer> ll1 = new LinkedList<>();
ll1.descendingIterator().forEachRemaining(ll1::add);

---------------------------------------------------------------------------------------
Integer[] a1 = {1,2,3,2,1};
Set<Integer> uniqueElements1 = new HashSet<>(Arrays.asList(a1));
--------------------------------------------Java 8 programming-------------------------------------
second smallest 	
  Integer[] a1 = {1,2,3,2,1};
        int a = Arrays.stream(a1).distinct().sorted().skip(1).findFirst().orElseThrow(() -> new Exception("no second"));
		int a = Arrays.stream(a1).sorted().skip(1).findFirst().orElse(0);
O/P - 2	

-----------------------------------------Common int of both array-----------------------------------------
int[] a1 = { 1, 2, 3, 4};
int[] a2 = { 3, 4, 5, 6};
	List<Integer> res = Arrays.stream(a1).filter(num1 -> Arrays.stream(a2).anyMatch(x->x==num1)).boxed().collect(Collectors.toList());
	System.out.println(":::::::::::::::::::"+res);
	O/P -----------  3,4

----------------------------------------------------------max length in given aray of string----------------------------------------
String ar[] = {"Amar","banana","sing"};
int max = Arrays.stream(ar).mapToInt(x->x.length()).max().orElse(0);
O/P ----- 6

String ar[] = {"Amar","banana","sing","fdgdrete"};
String g = Arrays.stream(ar).max(Comparator.comparing(aa->aa.length())).get();
O/P ----- fdgdrete

---------------------------------------Reverse using java 8------------------------	
int[] a1 = { 1, 2, 3, 4};
IntStream.range(0,a1.length/2).forEach(i->{
            int temp = a1[i];
            a1[i] = a1[a1.length-1-i];
            a1[a1.length-1-i] = temp;
        });
System.out.println("............"+a1);   -->> O/P -----  [4,3,2,1]

--------------------------------------------------------------partitioningBy-------------

Stream<Integer> s = Stream.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);   
// Using Collectors.counting() method  to count the number of elements in the 2 partitions 
Map<Boolean, Long> map = s.collect(Collectors.partitioningBy(num -> (num > 3), Collectors.counting())); 

map o/p: {false=3, true=7}   -->> less than 3 -> 3 numbers  and greater than 3 -> 7 numbers.

							And 
Map<Boolean, List<Integer> > map = s.collect(Collectors.partitioningBy(num -> num > 3)); 
map o/p: {false=[1, 2, 3], true=[4, 5, 6, 7, 8, 9, 10]}   -->> map.get(true)  map.get(false)

-----------------------------partitioningBy  Vs groupingBy----------------------------------------------------
1 - partitioningBy method will return a map whose key is always a Boolean value, but in case of groupingBy method, the key can be of any Object type

// groupingBy
	Map<Object, List<Person>> list2 = new HashMap<Object, List<Person>>();
	list2 = people.stream().collect(Collectors.groupingBy(p12 -> p12.getAge() > 22));
	System.out.println("grouping by age -> " + list2);
	
	o/P : {false=[Person List where age<22], true=[Person List where age>22]}

// partitioningBy
	Map<Boolean, List<Person>> list3 = new HashMap<>();
	list3 = people.stream().collect(Collectors.partitioningBy(p12 -> p12.getAge() > 22));
	System.out.println("partitioning by age -> " + list3);
	
	o/P : {false=[Person List where age<22], true=[Person List where age>22]}

2 - if you have a Predicate<T> instance, you can only pass it to Collectors.partitioningBy(). It won't be accepted by Collectors.groupingBy().
    if you have a Function<T/Employee,Boolean> instance, you can only pass it to Collectors.groupingBy(). It won't be accepted by Collectors.partitioningBy().
	
--------------------------------------------------------------------------------------------------------------
		List<Integer> l = Arrays.asList(10, 20, 30, 40, 50, 60, 70,80,90);
		AtomicInteger counter = new AtomicInteger();
        Collection<List<Integer>> ls = l.stream().collect(Collectors.groupingBy(it -> counter.getAndIncrement() / 3)).values();
		
		o/p: [[10, 20, 30], [40, 50, 60], [70, 80, 90]]       
		
			
-------------------------------------------------------------------functional iNterface-------------------------------------------
@FunctionalInterface  // optional
public interface MyFunction{
	void test(int i);
}

Main Class:

MyInterface face = (i)-> Sout("sjkfjsd");
face.test(15);

-------------------------------------------------------------selecting top salary employee for each department-----------------------------
List<Employee> employeeList = new ArrayList<>();

employeeList.add(new Employee("Mayur", "IT", "100", 1000));
employeeList.add(new Employee("Raj", "IT", "101", 2000));
employeeList.add(new Employee("Anshul", "IT", "102", 3000));
employeeList.add(new Employee("Hari", "EC", "102", 3000));
employeeList.add(new Employee("Ram", "EC", "102", 3000));

Map<String, Optional<Employee>> map = 
	employeeList.stream().collect(Collectors.groupingBy(Employee::getDepartment, Collectors.maxBy(Comparator.comparingInt(Employee::getSalary))));

O/P:	
IT=Optional[Employee{name='Anshul', department='IT', employeeID='102', salary=3000}]
EC=Optional[Employee{name='Hari', department='EC', employeeID='102', salary=30000}]

-------------------------------Print the number of employees in each department.----------------------------------------------------------------
Map<String, Long> countByDept = empList.stream().collect(Collectors.groupingBy(Employee::getDeptName,Collectors.counting()));

--------------------------------------------max age----------------------------------------------------------------------------------------
Optional<Employee> oldestEmp = empList.stream().max(Comparator.comparingInt(Employee::getAge)).get();

-------------------------------------------Find the department name which has the highest number of employees-----------------------------
Map.Entry<String, Long> maxNoOfEmployeesInDept = empList.stream().collect(Collectors.groupingBy(Employee::getDeptName, Collectors.counting())).
                                                 entrySet().stream().max(Map.Entry.comparingByValue()).get();
System.out.println("Max no of employees present in Dept :: " + maxNoOfEmployeesInDept.getKey());

--------------------------------------Find youngest female employee---------------------------------------------------------------------

Optional<Employee> youngestEmp = empList.stream().filter(e -> e.getGender() == "F")
                                  .min(Comparator.comparingInt(Employee::getAge));
Employee youngestEmployee = youngestEmp.get();

-----------------------------------------------------------------N'th Highest Salary Using Stream API--------------------
Map<String, Integer> employeeSalaries = ....

employeeSalaries.entrySet().stream() // Use Stream API to sort the entries by salary in descending order
            .sorted(Collections.reverseOrder(Map.Entry.comparingByValue())).collect(Collectors.toList()) // Collect the sorted entries into a List
            .get(n- 1); // Get the nth element from the list
			
------------------------------------------------------first non repeted---------------------------------
String sss = Arrays.asList(str1.split("")).stream().collect(Collectors.groupingBy(Function.identity(),LinkedHashMap::new,Collectors.counting()))
                .entrySet().stream().filter(entry-> entry.getValue() == 1).map(entry-> entry.getKey()).findFirst().get();

--------------------------------------------------first repeted char ----------------------------------------
String sss = Arrays.asList(str1.split("")).stream().collect(Collectors.groupingBy(Function.identity(),LinkedHashMap::new,Collectors.counting()))
                .entrySet().stream().filter(entry-> entry.getValue() > 1).map(entry-> entry.getKey()).findFirst().get();	

--------------------------------------------------------------------------------------------------------------------------			
List<Integer> numbers = Arrays.asList(10, 20, 30, 40, 50);
        OptionalDouble averageOptional = numbers.stream()
                                               .mapToInt(Integer::intValue)
                                               .average();
        double average = averageOptional.orElse(0.0);

---------------------------------------------------------------------------------------------------------

List<Employee> employees = Arrays.asList(
            new Employee("2025-07-14", "100"),
            new Employee("2025-07-14", "150"),
            new Employee("2025-07-15", "200"),
            new Employee("2025-07-15", "50")
        );

        Map<String, Integer> totalAmountByDate = employees.stream()
            .collect(Collectors.groupingBy(
                Employee::getDate,
                Collectors.summingInt(e -> Integer.parseInt(e.getAmount()))
            ));
---------------------------------------------------------------------------------------------------------------------	

public boolean isPalindrome(String text) {
    int forward = 0;
    int backward = length - 1;
    while (backward > forward) {
        char forwardChar = text.charAt(forward++);
        char backwardChar = text.charAt(backward--);
        if (forwardChar != backwardChar)
            return false;
    }
    return true;
}

List<Float> pricesList =  productsList.stream()  
                    .filter(p ->p.price> 30000)   // filtering price  
                    .map(pm ->pm.price)          // fetching price  
                    .collect(Collectors.toList());  

Stream.of("H", "T", "D", "I", "J").min(Comparator.comparing(String::valueOf)).get();
employees.stream().min(Comparator.comparing( Employee::getAge )).get();  // return Employee obj

Comparator --> functional INterface
	comparing(),reverseOrder -->> static method.
	reverse(), thenCOmpareing  -->> default method.


OptionalInt max = employeeList.stream().mapToInt(Employee::getAge).max(); 
if(max.isPresent())
System.out.println("Maximum age of Employee: "+max.getAsInt());

Map<String,Employee> a1 = list.stream().collect(Collectors.toMap(Employee::getName,e->e));

List<String> employeeNames = employeeList.stream().map(Employee::getName).collect(Collectors.toList());

List<String> reversed = names.stream()
                .sorted(Comparator.reverseOrder())
                .collect(Collectors.toList());
				
list.stream().sorted().collect(Collectors.toList());
List<Integer> uniqueElements = new ArrayList<>(
    sourceList.stream().collect(Collectors.toSet())
);
list.stream().distict().collect(Collectors.toList())

2 map with same with same key merge:

		HashMap<Integer, String> map1 = new HashMap<>();
		map1.put(1, "Ram");
		map1.put(2, "Rohan");
		map1.put(3, "Shivam");

		HashMap<Integer, String> map2 = new HashMap<>();
		map2.put(1, "Tushar");
		map2.put(10, "Satya");
		map2.put(12, "Sundar");
		
		map2.forEach(
		(key,value) -> map1.merge(key,value,(v1,v2) -> v1)
		)
		            OR 
		subjectToStudentCountMap2.forEach((key, value) -> subjectToStudentCountMap3.merge(key, value, (v1, v2) -> v1+v2)); 
		// add same key values o/p maths - 100 (map1 math = 50 and map2 math =50)

// Char count in string........................................		
String str = "abcaadcbcb";
Map<String, Long> charCount  = 
Arrays.asList(str.split("")).stream().collect(Collectors.groupingBy(Function.identity(),Collectors.counting()));
					OR
String input = "gainjavaknowledge";
String [] array = input.split("");
 
Map<String, Long> output = Arrays.stream(array).collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));

------
for (Map.Entry<String, List<TrafficBlock>> entry : filteredGroupedBlocks.entrySet()) {
                List<TrafficBlock> blocks = entry.getValue();
Set<String> dontDeleteBlocks = result.values().stream().flatMap(List::stream).map(TrafficBlock::getUuid)	

------------------------------------------------------------------------------------------------------------------------------------
Docker is like a chef who can cook one dish using a recipe.
Docker Compose is like a head chef who coordinates multiple chefs to cook a full meal (e.g., main course + side dish + dessert).

Docker
You can run one container at a time. But if your app needs MySQL, you have to manually run another container:
Then you have to link them, manage networks, and make sure they start in the right order. It gets messy.

Using Docker Compose: This starts both containers(springboot + MySQL) together, connects them, and handles everything for you.

--------------	Execution Plan----------------------------------------

Execution Plan / Query Execution shows how the Oracle Database will execute a SQL query. 
Performance Tuning: Helps identify slow queries and optimize them.
Index Usage: Shows whether indexes are being used effectively.
Join Strategy: Reveals how tables are joined (e.g., nested loops, hash joins).
Cost Estimation: Provides a cost metric to compare different query strategies.


EXPLAIN PLAN FOR
SELECT * FROM customers WHERE customer_id = 101;

does not return the actual query result. Instead, 
it tells Oracle to generate and store the execution plan for the specified SQL query in a special table called the plan table (usually PLAN_TABLE).


-----------------------------How oracle executes your SQL query to optimize?-----------------
1. Parsing - checks the SQL syntax and semantics. + It verifies object names (tables, columns)  + If the query is valid, Oracle generates a parse tree.
2. Query Optimization - Oracle's Cost-Based Optimizer (CBO) evaluates multiple execution strategies. + Key decisions made here:
		Whether to use an index or full table scan.
		Join order and join methods (nested loop, hash join, merge join).
		Use of parallel execution or materialized views.
3. Execution Plan Generation - Oracle creates a detailed execution plan based on the optimizer's decision.
4. Query Execution - Oracle executes the query using the chosen plan.
5. Fetching Results - The result set is returned to the client application.
----------------------------------------memory leak--------------------------------------------

A memory leak occurs when a computer program allocates memory but fails to release it
-Objects are created but never deleted.
Symptoms of Memory Leaks: Gradual increase in memory usage. + Crashes or out-of-memory errors.
How to Fix - VisualVM, JProfiler +  Code Review & Best Practices (Ensure proper use of destructors or finally blocks. + Avoid unnecessary global/static references.)

-------------------------------------handled Performance Issues--------------------
Slow queries
Missing indexes
Poor join strategies
Memory leaks
caching 
Load balancing issues
High response times
---------------------------------Difference between Horizontal scalability vs vertical scalability ?------------
Vertical Scalability (Scaling Up) - Increasing the resources (CPU, RAM, storage) of a single machine. -> Upgrading a server from 16GB RAM to 64GB RAM.
Horizontal Scalability (Scaling Out) - Adding more machines or Adding more servers 
-------------------------Difference between Array List, Hash Set & Hash Map ?----------------------------
1. ArrayList
	Type: List (ordered collection)
	Stores: Elements in a sequential order
	Duplicates: Allowed
	Access: By index (e.g., list.get(0))
	Use Case: When you need to maintain order and allow duplicates
2. HashSet
	Type: Set (unordered collection)
	Stores: Unique elements only
	Duplicates: Not allowed
	Access: No index-based access
	Use Case: When you need to store unique items	
3. HashMap
	Type: Map (key-value pairs)
	Stores: Key-value pairs
	Duplicates: Keys must be unique; values can be duplicated
	Access: By key (e.g., map.get("fruit"))
	Use Case: When you need to associate keys with values
	
---------------------------------------Difference between Synchronized and Locking ?----------------------
Synchronized
What it is: A keyword in Java used to control access to blocks of code or methods.
How it works: When a thread enters a synchronized block/method, it acquires a monitor lock on the object. Other threads must wait until the lock is released.
Scope: Can be applied to methods or blocks.
Simplicity: Easier to use, but less flexible.

Locking (Using Lock Interface)
What it is: A more flexible and powerful way to manage concurrency using classes like ReentrantLock.
How it works: Threads explicitly acquire and release locks.
Scope: Can be fine-tuned (e.g., try-lock, timed lock).
Flexibility: Allows interruptible and timed lock attempts, multiple condition variables.
	Lock lock = new ReentrantLock();	
	lock.lock();
	try {

Condition Support -> 	Synchronized (No)	 Locking ->Yes (Condition objects)	

Use synchronized for simple thread safety.
Use Lock when you need advanced control over locking behavior.

-----------------------------------------What happens when you don’t override hash code?--------------------
If you don’t override the hashCode() method in Java when you override equals(), it can lead to unexpected behavior, 
especially when using hash-based collections like HashMap, HashSet, or Hashtable.
	Objects that are "equal" (via equals()) may end up in different buckets due to different hash codes.
	Duplicate entries in a HashSet
    Failure to retrieve values from a HashMap

Ex
HashSet<Person> set = new HashSet<>();
set.add(new Person("Ravi"));
System.out.println(set.contains(new Person("Ravi"))); // ❌ Might return false

-----------------------How many copy we have in static keyword ?----------------------
In Java, when you use the static keyword, it means the member (variable or method) belongs to the class itself, not to any individual instance of the class.
So Only one copy per class, regardless of how many objects (instances) you create.
------------------------------------------What is aggregation and composition--------------------
aggregation and composition are two types of association that describe relationships between objects. Both represent a "has-a" relationship, but they differ in ownership
Aggregation
Definition: A weaker form of association where one object "has-a" reference to another, but they can exist independently.
Example: A Department has Employees, but an Employee can exist without a Department.

Composition
Definition: A stronger form of association where the child object is owned by the parent.
Example: A House has Rooms, and if the House is destroyed, the Rooms are too.

---------------------------------------------Components in aggregations------------------
1. Aggregator (Parent Class)
This is the class that contains or references other objects.
It does not own the lifecycle of the contained objects.
Example: Department in a university system.
2. Component (Child Class)
These are the objects that are part of the aggregator.
They can exist independently of the aggregator.
Example: Professor, Student, or Course in a Department.
--------------------------------------------------------------------Memory leak exception ?---------------------
A Memory Leak Exception typically refers to a situation where a program consumes increasing amounts of
memory over time due to unreleased resources, eventually leading to OutOfMemoryError
1 - java.lang.OutOfMemoryError
2 - StackOverflowError - Can occur if memory leaks are caused by uncontrolled recursion.

--------------------------------------------------------How many parts are there in JWT ?---------------
A JWT (JSON Web Token) consists of three parts, each separated by a dot (.):
1. Header -> Contains metadata about the token. -> The type of token (JWT) + The signing algorithm used (e.g., HS256, RS256)
2. Payload
	Contains the claims or data.
	Can include:
	Registered claims (e.g., iss, exp, sub)

3. Signature
Used to verify the token’s integrity and authenticity.
Created by encoding the header and payload, then signing it with a secret or private key.
	

-----------------------------------------------------------------------------------------------------------------------------

	
Polymorphism means "many forms". In Java, it allows a single interface/method to behave differently based on the object that is calling it. overloading and overriding.

Encapsulation is the process of wrapping data (variables) and code (methods) together into a single unit, 
typically a class. It helps protect the internal state of an object from unintended or harmful changes.

Inheritance allows a class to acquire properties and behaviors from another class. It promotes code reuse and establishes a parent-child relationship.

Abstraction hides complex implementation details and shows only the essential features of an object.
----------------------------------------------------------------ELK ---------------------------------------------------------------------------------------------
elastic serarch is a nosql DB that is based on Lucene search engine will help us to store inputs/logs
Logstash: is a log pipline toole that accepts inputs/logs from various sources and exports them to various targets.  --> command: logstash -f logstach12.conf
KIbana : ui to monitor app logs.  -->> need to create index-pattern where we mapped the index(created in log.conf file inside output tag.)
logstach12.conf -->>
	input{
	  file{
	     path=> "C:\User...  (path of log file.)
		 start_position => "beginning"
		 }
	}

    FIlter{
	  if user want to keep only selected log.
	}
	
  output{
  elesticsearch{
     hosts => ["localhost/url"]
  }

application.yml
	logging:
		file: C:\User...
		
------------------------------------------------------------system design---------------
COmponent fo system design : 
	Load Balancer, Caching, API Gateways, Monitoring System, Rate limiters, Distributed logging services etc.

System Design Life Cycle | SDLC (Design)
1. Planning Stage
2. Feasibility Study Stage  --> Asses the practicality of the proposed system. like cost.
3. System Design Stage      -->> Develop a blueprint of the system architecture and components.
4. Implementation Stage
5. Testing Stage
6. Deployment Stage
7. Maintenance and Support	

Vertical scaling : , also known as scaling up, refers to the process of increasing the capacity or capabilities of an individual hardware or software component within a system.
Horizontal scaling:  also known as scaling out, refers to the process of increasing the capacity or performance of a system by adding more machines or servers 
                                 
---------------------------------------------Whatsup---------------------

Non-Functional Requirement
Low latency: 
Consistency: 
Availability: 
Security: 
Scalability: 

Functional Requirement
Conversation: The system should support one-on-one and group conversations between users.
Acknowledgment: 
Sharing: The system should support sharing of media files, such as images, videos, and audio.
Chat storage: The system must support the persistent storage of chat messages when a user is offline until the successful delivery of messages.
Push notifications: The system should be able to notify offline users of new messages once their status becomes online.

The following steps describe the communication between both clients:
	User A and user B create a communication channel with the chat server.
	User A sends a message to the chat server.
	Upon receiving the message, the chat server acknowledges back to user A.
	The chat server sends the message to user B and stores the message in the database if the receiver’s status is offline.
	User B sends an acknowledgment to the chat server.
	The chat server notifies user A that the message has been successfully delivered.
	When user B reads the message, the application notifies the chat server.
	The chat server notifies user A that user B has read the message.

Data Model Design:
	users: This table will contain a user's information such as name, phoneNumber, and other details.
	messages: This table will store messages with properties such as type (text, image, video, etc.), content, 
	and timestamps for message delivery. The message will also have a corresponding chatID or groupID.
	chats: This table basically represents a private chat between two users and can contain multiple messages.
	users_chats: This table maps users and chats as multiple users can have multiple chats (N:M relationship) and vice versa.
	groups: This table represents a group between multiple users.
	users_groups: This table maps users and groups as multiple users can be a part of multiple groups
		
---------------------------------Load Balancer-------------------------------
Scalability , Performance, Avalalibility.

Ever time we should have 2 LB one is primary and second is backup, when primary is down backup can redirect the req to server.

Global LB : if app in multiple geo location like twitter face book.  user -->> global LB -->> find the correct datacenter and  -->> local LB
Local LB: if your app in one data center or one geo locaton	

How LB Distributes the request:
 1- Round robin scheduling: in this each request is sequenclly forward ex if we have 3 server: first 1  then 2 then 3 then again 1,2,3
 2- Weighted round robin: it is perfer to use this when we have better confiruged server those can handle more than 1 req. so in this each node/server s=given a node.
                          so req from client forwarded by LB based on node weight ex if server 1 has weight 2 then --> 1,2 req go to server 1 
						  3rd req to server 2 and 
						  req 4 send to server 3
3- Least Connection: suppose some req takes long time to fullfil, in this case might be server 1 will be busy to handle second request and the same time server 2,3 might
                      be less connection and ready to take req.
                      In this case we can use this algo like least connection where server with less connectio give the priprity to new req.
					  
How to implement LB :
			Depend on incoming request and need various LB type can be implemented:
			   1 - Hardwer LB: singel physical server dedicated fro LB task and standlone. very expensive but very effective/performace. + need copy of this on failur so another cost.
			   2 - Software LB: where sigle server doing some work + LB work so seperate server for LB. effordable + scalable (increase the no tof server when trafic increace)
			   2 - Cloud LB: above 2 user has to maintain and extra work why LB is down when it down. pay as you need. LB serivce provide by cloud owner like azur, aws.
			   SO if you have your app in-permises , can use s/w LB or if you are already using cloud serivce then better to use Cloud LB.
			   and if you are not vary of cost and want good performance the go for H/W LB.
			   
Note: Spring cloud load balancer use spring cloud ribbion whihc use round robin algo.	
      for springboot project we need to add ribbion dependecy if we want client side LB(user app configure the LB).		

1  -	  
@SprinBootApplication
@RibbonClient(name="chat",configuration=RibbonConfiguration.class)

@Bean
@LoadBalanced   
public RestTemplete restTemp(){
return new RestTemplete();
}	

2  - create class and create 2 Bean. IPing and IRule.
class RibbonConfiguration(){
 @Autowired
 IClientConfig ribbonClient;   -->> coming from netflix.client.config.
 
 @Bean
 public IPing ping(IClientConfig ribbonClient){
 return new PingUrl();
 }
 
 @Bean
 public IRule rule(IClientConfig ribbonClient){
 return new AvailabilityFilterRule();
 }
 
 // IPing and IRule are coming from netflix.loadbalancer 

}  

3 -  application.yml
chat:    --> appname of server which instance will be called. which multiple instances are running.
  ribbon:
    ereka:
	  enable: false
    listOfServers: localhost:8081,localhost:8082
	ServerListRefreshInterval: 2000

4 - COntroller
@Autowired
RestTemplate template;

@Getmapping("/invoke")
String url = "http://chat(appname)/....."      -->> this appname will go to yml file and check the listOfServers/instances
return template.getForObject(url,String.class);

-------------------------------------To implement LB in eureka---------------
just add the dependency <spring-cloud-starter-netflix-ribbon>  in calling microservice 

suppose shopping app is calling the payment service  OR  shopping -->> payment (payment app running in mulpiple ports and all are register in eureka server.)
then we add this dependency in shopping service (it is also a eureka client)

In cloud Eureka we did not configure IPing and IRule and IClientConfig but still it will auto provide the LB with Ribbon Client 

---------------------------------------------Feign Client----------------------------------- openfeign starter dependency--
develop by netflix to consume external service .
replace restTemplate with Feign client it is easy just declare the interface and no need to write Junit

1 - Create one interface:
	@FeignClient(url="http://abc.com", name="user")  -->> url is the endpint which we want to call. and /user will will add in method
	public interface UserClient{
	
	@GetMapping("/user")           -->> this will add in url and become the complete endpoint -->> url="http://abc.com/user"  --> this URL is returing the List<User>
	public List<User> getUser();   -->> here only method signature so no need of junit
	
	//here we can add more abstract method which will append to same URL
	
	}

2 - Controller 
	@RestController
	@EnableFeignClients
	{
		@Autowired
		private UserClient userClient;
		
		@GetMapping("/findAllUser")           
	     public List<User> getAllUser(){
		  return userClient.getUser();
		 }
		 
	}

3 - add openfeign starter dependency

---------------------------------------------- java.util.function package------------------------------------
Supplier: It does not accept and input but return value.
	Supplier<String> supp = () -> {
		return String after processing and list or string;
	}

	supp.get();  // this will execute our suplier and give String.
	

Predicate: 
		1 - Predicate<TrafficBlock> pre = xyz();
		2 - Predicate<TrafficBlock> xyz(){
							 return trafficBlock -> {
								operate with trafficBlock obj and return TRUE/FALSE
								}
							 }
		3 - pre.test(trafficBlock);  // actual test the method.

The Consumer :
         interface has one abstract method accept(T t), which takes an argument of type T and performs the operation on it.		
		 It’s used in scenarios where you want to process the collection or execute some action on objects without returning anything.
	public void consumerExample() {
			Consumer<String> consumer = (name) -> System.out.println("This is my name ->  " + name);
			consumer.accept("bhairab");
		}

BiConsumer in java8
	This interface takes two input arguments and performs some action without returning any result.
	The BiConsumer interface has a single method called accept(T t, U u) that takes two arguments of types T and U and performs the desired operation on them.
	
The Function 
	interface has a single abstract method apply(T t), which takes an argument of type T and returns a result of type R.	
	public int findLength() {
        Function<String, Integer> findLength = (text) -> text.length();
        int strLength = findLength.apply("bhairab");
        return strLength;
    }
	
Note:       -->> when to use Supplier.
	Suppose you have parameters stored in database that you want to keep in constant all over your app

	// Assume retrieveSystemParameter query database which allows to change parameters
	public static String SYSTEM_PARAMETER = StaticUtilities.retrieveSystemParameter();  // That value will be initialized once and won't change untill a redeployment.
		
	if instead you use a supplier :
	public static Supplier<String> SYSTEM_PARAMETER_SUPPLIER = StaticUtilities::retrieveSystemParameter;	
		When you need the value somewhere you will call SYSTEM_PARAMETER_SUPPLIER.get() which will retrieve parameter in the database when needed -
		that way if any change in database, you won't have to redeploy.
		As you can see, Suppliers are lazy. They do the work when you ask them to work (by calling .get())
		
	2- 
		final Product firstProduct = Optional.ofNullable(product)       -->> this will execute orElse everytime, does not matter producr is null or not, in last it will return the value.
			.orElse(productDao.findProductById(id));

		final Product secondProduct = Optional.ofNullable(product)   -->> oeElseGet will execute only when product is null
			.orElseGet(() -> productDao.findProductById(id));
		

-------------------------------------------------------	Deadlock--------------------------------	

2 - Acquire locks in a consistent order: If multiple threads need locks on multiple objects, make sure they always acquire the locks in the same order. 
	process1(){
		thread1.lock()// syncroze(resource1)
		thread2.lock()// syncroze(resource2)
	}
	process2(){
		thread1.lock() // syncroze(resource1)
		thread2.lock() // syncroze(resource2)
	}
	
Release locks promptly: When a thread has finished using a shared resource, it should release the associated lock as soon as possible. 
	It allows other threads to acquire the lock and continue their execution.
1 - Use timeouts: When acquiring locks, you can specify a timeout period. If the lock cannot be acquired within the specified time, 
	the thread can release the lock and try again later.
Use a single lock: If possible, use one lock to protect multiple resources. In order to avoid deadlock, it makes sure that only one thread can access the resources at once.
Use thread-safe classes: Instead of implementing your synchronization mechanism, use the thread-safe classes provided by the Java API. 
	These classes have already been designed and tested to handle multi-threading correctly.
3 - Avoid nested locks: Avoid acquiring locks on multiple objects in a nested fashion. It can lead to deadlocks, 
	increasing the likelihood of two or more threads acquiring locks in a different order.
Using Thread.join() Method: 

-------------------------------------------------------------inter communication-------------------
when a thread communicates with another thred by sending some singals to ask a thread to wait of wake up another thread from sleep, this is done by inter thread communication.
obj.wait() : it tell the current thred to go to sleep and relese lock and wait until other thread call notifiy notifyAll().should be in syncronize blocks
this.notify(): wakes up a thread that called wait on the same object.
this.notifyAll : it wakes all the thread on which wait was called on the sam eobject.

		example class: 
		int sum = 0
		run(){           
			for(){
				calculate sum of 100 num
			}
		}
		
		main class:
		PS void main(){
		Example ex = new Example();
		ex.start()
		
		sop(sum)
		}
o/p sum = 0  // because thread main thread will not wait for thread output and print 0 and thread need time to make sum of 100 number.
		
		fix: 
		
		example class: 
		int sum = 0
		run(){ 
		synchrozed(this){             <<<<<-----
			for(){
			}
			this.notify()               <<<<-----
		}
		print sum of 100 num
		}
		}
		
		main class:		 throws IntrruptedEx
		Example ex = new Example();
		ex.start()
		synchronized(ex){
		ex.wait()
		sop(sum)    -->> 1275
		}
		
		}
------------------------------------------------------------thread share resource----------------------

static int i =0;
i++ -->>   3 step ; 1- get i value; 2 - increment 3- assign value;

For example, consider a simple increment operation on a variable. This operation is not atomic in nature, meaning it involves multiple steps such as reading the value,
 incrementing it, and then writing it back. If two threads are performing this operation simultaneously on the same variable, they might both read the value at the same time,
 increment it, and then write it back, resulting in the variable only being incremented once instead of twice.

To prevent these kinds of issues, you can use synchronization. Synchronization ensures that only one thread can access the shared resource at a time. 
It maintains the consistency of shared data.



------------------------------------------------------------------MetaSpece---------------------------------------------------		
Some of the areas are created by the JVM whereas some are created by the threads that are used in a program execution.
The following image illustrates the different memory areas in Java: 
	Heap area, method area,	JVM stack, Native method stack, PC register.	

Here, the heap area is one of the most important memory areas of JVM. Here, all the java objects are stored. 
The heap is created when the JVM starts. The heap is generally divided into two parts. That is:

Young Generation(Nursery): All the new objects are allocated in this memory. Whenever this memory gets filled, 
   the garbage collection is performed. This is called as Minor Garbage Collection.
Old Generation: All the long lived objects which have survived many rounds of minor garbage collection is stored in this area. 
   Whenever this memory gets filled, the garbage collection is performed. This is called as Major Garbage Collection.
   
PermGen Memory: 
	This is a special space in java heap which is separated from the main memory where all the static content + application metadata required by the JVM store in this section. 	
	The biggest disadvantage of PermGen is that it contains a limited size which leads to an OutOfMemoryError. 
	The default size of PermGen memory is 64 MB on 32-bit JVM and 82 MB on the 64-bit version. 

MetaSpace grows automatically by default. Here, the garbage collection is automatically triggered when the class metadata usage reaches its maximum metaspace size. 
	Metaspace by default auto increases its size depending on the underlying OS.
	
	The term “Meta Space” in Java refers to the area of memory allocated for the Java Virtual Machine (JVM) to store class metadata + static content.
	
The MetaSpece use native memory and the organization in memory with pointers makes that the GC are faster than older PermGen memory.
	
-------------------------------------------------------------------------------------------------------
1 - 
@PostMapping("/login")   // this call will be disable by @EnableWebSecurity and SecurityFilterChain securityFilterChain(HttpSecurity http)  -->> http.csrf().disable()
createAuthenticationToken(user,pass)

		---->>>>  
	     authenticationManager.authenticate(new UsernamePasswordAuthenticationToken(user,pass));  // auth the user and pass from DB/UserDetails Serivce.

        final UserDetails userDetails = userDetailsService.loadUserByUsername(authenticationRequest.getUsername());
        return jwtUtil.generateToken(userDetails); // createToken from JWTUtils wih our own SignatureAlgorithm
		
2 - 
@PostMapping("/signup")
public ResponseEntity<?> getDetails(@RequestBody User user) {  //  this call is not filter so will go to doFilter
    service.getUserDetails(user)

	calss extends OncePerRequestFilter{
	
		@Override
    protected void doFilterInternal(HttpServletRequest , HttpServletResponse, FilterChain chain)
	  if (header != null){
	       autionHeader.startsWith("Bearer ")) {
            jwt = authorizationHeader.substring(7);
            username = jwtUtil.extractUsername(jwt);
        }
		}
		
		// it will validate the token if correct then set it to UsernamePasswordAuthenticationToken and set to SecurityContext
	chain.doFilter(request, response);
	
// Then if user authtecate successfully then it call the service.getUserDetails(user);

----------------------------------------------------API gateway authentication Strategy------------------------------------------------------

An API gateway is a software layer that sits between your backend services and your API clients. 
It acts as a reverse proxy, routing requests from clients to your backend services and returning the responses back to the client.
One common use case for an API gateway is to provide an additional layer of security for your backend services.
	This can be achieved through various forms of authentication, including :
	   
Note: API gateway authentication is important because it helps to ensure that only authorized clients are able to access the microservices behind the API gateway. 
	   
1- Basic API authentication:
	With basic authentication, a client sends an HTTP request with a username and password encoded in base64. Typically, the API gateway validates the username and password 
	against a predefined list of users and passwords.
	
2- Key-based authentication:
	With API key authentication, a client send a unique key in the request header or as a query parameter, and the API gateway checks that the key is valid. 
	API keys can be generated and managed by the API provider or by an external system like JWT. This approach is useful for HTTP APIs.

3- LDAP authentication:
	LDAP (Lightweight Directory Access Protocol) is a widely used protocol for storing and querying authentication information. With LDAP authentication, 
	the API gateway can validate client credentials by checking them against an LDAP server, which acts as a central repository for user information.

4- OAuth2 authentication:
		With OAuth 2.0, a client obtains an access token from an authorization server, and then includes that token in each subsequent request to the API gateway. 
		The API gateway can then validate the token from authorization server and determine the client’s level of access. 
		This can be useful in situations where you want to give third-party services limited access to your API. However, it only works with HTTPS requests.

5- OIDC authentication:
	OpenID Connect (OIDC) is a widely used standard built on top of OAuth 2.0. It provides a way to authenticate clients and obtain user information in a single request. 
	With OpenID Connect, a client obtains an ID token + access token from an authorization server (it contain Open id connect provider + UserInfo endPoint), 
	OpenID Connect can be useful in situations where you want to obtain user information in addition to authenticating clients.
	
So we can make sure that the gateway will only allow authenticated requests to be routed . 
It can be easily integrated with identity and access management tools like keycloak or identity server 4.
	
-----------------------------------------------------------------------------------------------------------------------------------------
Java 8 gives concisness in the code.
	functional programing wihci is enable by Lambda (powerful tool to create concise code base.)
Lambda express is a anonymous function i.e without name, return type and acccess modifier.

New in java 8: 
	Lambda expression. stream api, default and static, functional interface, Optional, method ref etc.

Note: Predicate - in this we can use and/or
	Predicate<String> first;
	first.add/or(predicateSecond).test("testing string");)
	
	Function: andThen and compose()
	 funtion1.andThen(function2).apply(Input) -->> fist funtion1 wxecute the funtion2
	 funtion1.compose(function2).apply(Input) -->> fist funtion2 wxecute the funtion1
	
	Consumer:
	andThen() only no compose()

Stream is a special iterator class that process the element of collections

-----------------------------------short circuit operators-------------------------------
these operator skip the processing. compiler will not execute whole like in && second condition will not execute if first is false, same OR will skip 2nd if first true.
	Before java 8 we have 2:  && or || : boolean short circuit evaluation.
	in java 8 we have: Terminal operator: findFirst,findAny, anyMatch(), noMatch()
					   Intermidiate: limit()
					  
-------------------------------------------Peek() and reduce()----------------------------------------	
peek() is a itermediate operation it takes a consumer obj as input.
peek() exist maily support to debugging. where we want to seee element during stream pipeline flow	
	list,strea().filter(a->a%2==0).peek(Sout).map(a->a+a).filter(a->a>5).count();
		so here if we want to see the element after first filter then we can see using peep()

Reduce:
	stream.reduce(): combine the elements of stream and produce a singel value. it apply to a binary operator where first input is return value and second is current stream element.
		ex: list(1,2,3,4,5)
		    list.stream.reduce((a,b) -> a+b).get();\\ this will add the list value and reutn 15
			
---------------------------------------------------print duplicate element in list-----------------------
list : 10,10,1,1,9,8,7,6
HashSet h = new HashSet<>()
list.sream().filter(x-> !h.add(x)).collect(Collectors.toSet());

-------------add 2 new with user define functional iNterface-------------------------------------------
@FunctinalInterface
class iFunc{
int add(int a, int b);
}



iFunc fu = (a,b)-> a+b;
int result = fu.add(1,2);

-----------------------------------limit() && skip()------------------------------
Both are intermidiate: 
limit(n) it return not longer than given n. first n nnumbers
skip(n): it skip the first n numbers; skip first n numbers.

-----------------------------number prime or not------------------------
return number>1 && Intstream.range(2,number).noneMatch(n->number%n ==0);

--------------------------get all int who start with 3----------------------------------------------------------------------------------
List<Integer>  list = Stream.of(21,31,35,56,34,300).collect(Collectors.toList());
        System.out.println(list.stream().filter(x-> Integer.parseInt(x.toString().substring(0,1)) == 3).collect(Collectors.toList()));
        list.stream().filter(x-> Integer.parseInt(x.toString().substring(0,1)) == 3).collect(Collectors.toList());
--------------------------------------------------------------------------------------------------------------------------------------------------		
		
Hashing in hasmap means using some algo/function to map object with same int value.
Hashmap contains sub class Node:
						final int hash;
						final K key;
						V value;
						Node<K,V> next;
						
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);    -->>hash -- > somthing like key%10 (if key = 5 then 5%10 = 5)
}	

(treefy=8)threshold = 8 after that linklist dynamically  convert to b-tree.		 -->> o(log n) -- bettter than	  old one : O(n) 
some : on threshold(untreefy) = 6 tree converted to linklist

HashSet:
		1,2,3
		Key = 1 ; value = static final Object PRESENT = new Object();
		
-----------------------------------Optional-------------------------------------------------
optional is singel value container which contain vaule or not.	

------------------------------------------deploymnet strategies---------------------
recreate:
    is this old instance will down and start the new instance.
rolling:
  suppose we have 5 desired instance and max-Surge = 1(1+ to desired = 6) then it will start 1 and once it is healthy then total 6 
  and it will delete the old one and now total will be 5 again. it will do it same for all.

Blue and green:
   here maintain to identical environment 1 -Blue (current live version) and 2- Green (new version going to deploy).
   Traffic is switched to greenonce its verified as stable and load bancer will move from Blue to green.
   And traffic can easily revert to Blue is issue dected, by a quick roll back enable.
   
asynchronously Transaction -->> Saga.
synchronization way:
-------------------------------------2 phase and 3 phase Transaction-------------------->> 
2 phase: one co-ordinator will coordinate with participants: order(coordinator)  -->> inventory --> payment
	1 - prepare (once participants ready send yes or NO)
	2 - commit and roll back -->> then Acknowledgment

3 phase: one co-ordinator will coordinate with participants: order(coordinator)  -->> inventory --> payment
	1 - prepare (once participants ready send yes or NO)
	2 - pre-commit  extra check (participants accquire with lock and other)
	2 - commit and roll back  -->> then Acknowledgment
	
---------------------------------------Questions--------------------------------------------------
Microservice: where we building our app by breaking down into small chunk. and deploy then seperatly.

-----------------------------communication-------------syns and async-----------
rest template: for simple or direct communication b/w services
Feign client: when we are dealing with multiple services, It make clean code.
asynchronous: message brokers: kafka / rabitt MQ. 

----------------------------caching in spring boot-------------------------------
1- spirng-boot-starter-cache
2 - @EnableCaching
3 - @Cacheable on method  + optionally : to custimize the cache behaviour we can use @CacheEvict/@CachePut

cache evict vs cache expiration: 
	evict: when data is removed from cache.
	expiration: data is removed because it is  too very old.
	SO eviction manage cache size while expiration ensure fresh data.
	
@Cacable(key="#id", value="deptIdCache")  // here id will be same as parameter and value can be anything.
public getDetById(long id){
return ..
}
               OR  -->> if we want to load all data to cache. at server start
			   
@Cacable(key="#id", value="globalCache")  // here id will be same as parameter and value can be anything which we use in config class.
public getDetById(long id){
return ..
}

create config class{
Autowired
CacheManager cacheManager;

@PostConstruct
loadCache(){
 Cache cache = cacheManager.getCache("globalCache"); // globalCache this use in controller inside @Cacable
 //get all dept from DB and set in cache by Key Value pair;
 for{
	cache.put(dept.getID,department);
 }
 
// when delete limited cache entry
@cacheEvict(value="globalCache", key="#id")
public void clearCache(long id){

}

// when delete all cache entry
@cacheEvict(value="globalCache", allEntries=true)
public void clearCache(){

}
------->>>>>> 
spring.cache.type=redis   or EHCache   /  default cache -> spring.cache.type=simple and @Cacheable("users")

spring.redis.port = 6379
spring.redis.host = localhost


@scheduled(fixedRate=15000, initialDelay=15000) // fixedRate every 15 sec this run and initialDelay of 15 sec from app started.

-----------------------------------------------------Redis Cache--------------------Redis && Jedis(java impl for redis.)------------------------

--------------------------------When You Don't Need RedisTemplate---------------------------------
If you're only using Redis for caching via annotations like @Cacheable, @CachePut, and @CacheEvict, Spring Boot handles everything internally. You just need:

spring-boot-starter-data-redis
spring-boot-starter-cache
@EnableCaching in your main class
Redis running and configured in application.properties
Spring Boot will auto-configure the cache manager using Redis.

------------------------------------------------------When You Do Need RedisTemplate----------------------------------------
You should use RedisTemplate if you want to:

Interact with Redis directly (e.g., set/get keys manually)
Store custom objects with serialization
Use Redis for more than just caching (e.g., pub/sub, distributed locks, queues)

-----------------------------------Using--RedisTemplate-------------------------------------------------------------
1 - Below code to save data in Redis and fetch from that.
@Bean
public JedicConnectionFactory connectionFactory(){
	// configure host and port for Redis
}

@Bean 
RedisTemplete<String, Object> redisTemplete(){
	RedisTemplete<String, Object> redisTemplete = new RedisTemplete<String, Object>();
	redisTemplete.setConnectionFactory(connectionFactory());
	return redisTemplete;
}

Repo:
 @Autowire 
 RedisTemplete redisTemplete
 HashOperations hashOp = redisTemplete.opsForHash();
 
 save / Update: 
   hashOp.put("USER",user.getId(),user);  //USER = some name/hash value; user is the pojo comint to save.
   
 getAll:
   hashOp.entries("USER")  OR  hashOp.values("USER")
 
  delete: 
    hashOp.delete("USER",id)
	
   get: 
     hashOp.get("USER",id)

2 - Then Use @Cache to use cache of Redis so no need to go to DB again.

@cacheable(Key="#id", value="product", unless="#result.price> 100") // so cache only those whose priceis less than 100 ro dont cache if price bigger than 1000
getUserById()
	
@CacheEvict(key="#id", value="USER")	 // USER is hash value which we define in Repo.	
deleteUser()

@CachePut(key="#user.id", value="USER")
saveUser(User user)

-------------------------------performance issue with high load. how identify and address---------------
1- I would identify the performance issue using toole springboot actuator or splunk.
2 - analysis app log for error  on high load.
3 - start performance test use any profiler/jmeter
4 - after finding issue i can use caching, optizize DB or use scaling options

----------------------best practice for versioning in rest api--------------------- 
When client requirements change and we dont want to modify existing service.

1- URL versioning: include the version no in URL : /api.api/v1/products   OR /api.api/v2/products     //  Twitter use this
2- req parameter versioning:  /api.api/products?version=1  OR /api.api/products?version=1             // amazon

3- header versioning  : 
    Using Accept Header:
		URL will not change. In Accept header we will tell the version of api: Accept:application/app.v1.categories.
	Using CUstom header:
		use custom property to set version number: headers = [X-API-VERSION=1]  //X-API-VERSION = custom property.
	
4- Media Type Versioing 

okay java
---------------------------------@EnableAutoConfigration---------------------
it tell the framwork to auto set-up the application based on dependencies
springboot internally uses condition evaluation, examine the classpath , existing beans and properties.

-----------------actuator endpoint--------------
It is a tool for monitoring and managing the app. it give us the endpoint where we can check health, view configuration, beans etc
It is super useful for keeping an eye on how our app is doing.

In prod env these endpoints can reveal sensitive info about your app so need to restirct it 
   How?
		1 - Limit Exposure: by default ot aal endpints are exposed, we can control whihc one can be available over the web.
		2- use spring security: we can configure spring security to require authentication for accessing the endpoint.
		3 - Actuator ROle: create specific role like Actuator_Admin/and assign endpint access as per there role.
		
-------------------------------------------------Strategy to optimize the performance------------- 
1- Implement caching for frequent accessed data.
2- optimize DB queries                          
		Reduce the use of wildcard characters, 
		Use indexes effectively,
		Use EXIST() instead of COUNT() queries - When searching for a specific element in a table, it’s more efficient to use an EXIST() keyword instead of a COUNT() one
		Avoid subqueries - When subqueries are used in WHERE or HAVING clauses, they can slow down the performance of the query. JOIN clauses are often a better choice. 
		Avoid SELECT * and retrieve only necessary columns
		
3- use asynchronous methods for opertaion like sending emails.
4- load balancer if traffic high.
5 - optimize the code. USe @Async
6- Use WebFlux to handle a large no of concurrent connections. -- Project Reactor recommended for springboot for reactive progrea

------------------------------------------------------reactive programming----------------------------------------------------------

Spring WebFlux is a reactive programming framework designed to handle the demands of modern, event-driven, and non-blocking architectures. 
Its main advantage lies in its ability to handle a large number of concurrent connections with lower resource consumption compared to traditional synchronous frameworks.

Spring WebFlux can be defined as the reactive programming framework provided by the Spring ecosystem for the building of asynchronous, non-blocking, 
and event-driven applications and it can be designed to handle a large number of concurrent connections while consuming less resources.

Mono and Flux: Both are publiser , these 2 project reactor datatype of ; which work with reactive programming.
	
@Repository
public interface UserRepository extends ReactiveCrudRepository<User, String> {
}

public Flux<User> getAllUsers() {
        return userRepository.findAll();
}
public Mono<User> getUserById(String id) {
	return userRepository.findById(id);
}

Note: 
  Traditionally REST API: first req come to server and thread-1 assign to req-1 and it process and to DB and thread-1 will blocked untill it get respose from DB.
	Second req come to server and thread-2 assign to req-2 and it process and to DB and thread-2 will blocked untill it get respose from DB.
	request ....... n
	And we have a Thread poll limitation if if we suppose we have Thread-poll = 20 in Server then 21 req will be block untill any Thread get free.
 
 Reactive world:
   eleminate thread per req concept. 
   In this Thread-1 process the req-1 and send to DB but wont wait for res, just send event to say to DB do your job once complete assign to some free thread 
   and publish the complete event. In this case thread is free and ready to handl enext req.
   SO with less thread we can proess tons of requests.
  

Reactive stream specification: interface. 4 pillor of reactive programming
1- Publisher : publish an event (DB driver)     -->> subscribe(Subscriber)
2 - subscriber/consumer: sub  to event (backend/browser)     -->>  onSubscribe(Subscription s); onNext(T t);// 10 records then 10 onNext() call
							OnError(Throwable); onComplete() // if fail OnError() else onComplete will execute.
3- Subscribtion: represent the unique relationship b/w publiser and consumer   -->> request(n); // get the data from publiser. cancel()
4- Processor:  represent a processing stage in both pub sub

Steps:
Subscriber/COnsumer -->> call-->> Subscribe() on PUbliser
Publiser         ---->> call Subscribtion() on COnsumer     //publiser say subscribintion is successful
consumer -->>   call Request(n) on PUbliser       // here if consumer call request(2) then Publiser will return only 2 out of n records so sonsmer can restrict that // BackPressure
publiser -->> call onNext(data) on Consumer               // publiser, publish data by this method
publiser -->> call onError()/ONComplete() on COnsumer.

Backpressure: When DB return large data and restApi may nao handel that and return outOfMemoryError in synchronized world on BackPressure feature. but in Reactive
               It is there, so when DB return large data then this feature tell to DB dont push all the data first i will process what i have, once finish that only give other data.
			   So BackPressure is, adding a limitation on DB driver, like how many data you are expecting.

Library -->> Project Reactor: use by SpringBoot for this.

Mono<String> mono = Mono.just("Hello");  //Mono<String> mono = Mono.just("Hello").log(); // all method can see.
mono.subscribe(sout)   // MOnO is publiser so call on that subscribe()

Flex<String> flex = Flex.just("sd","sd","sdfsdf") // 3 string.
flex.subscribe(sout); // 3 onNext() event will be call.

flex.error() //

Dao::::::::
Flux<Customer> getCustomer(){
return Flux.range(1,10)
 .delayElements(Duration.ofSeconds(1)
 .doOnNext(i->Sout(",..."))
 .map(i-> new Customer(""));

COntroller: 
	@Getmapping(value="/",produce = MediaType.TEXT_EVENT_STREAM_VALUE) //without this mediaType, this will behave like blocking only,we need to send data to browser in as a event Stream.
	public Flux<Customer> getAll(){                                    // rather as a object.
	return dao.getCustomer();
	}

Functional Endpoint---------------------------------------->>>>>>>>>>>>>>>>>>>>>>>>>>>
REST Api                                                         VS                  Functional Endpoint----------  Browser -> Router -> Handler
browser send req to COntroller and controller send to Service                       Brower send to Router and Router redirect req  to corresponding Handler

Note: we can write both Functional endpoitn and Rest api for spring reactive.

Router------>>>>>>>>>>
@Confuguration
Class RouterCOnfig{

priavate CustomerHanler customerHanler;

priavate AnotherHanler anotherHanler;

@Bean
public RouterFunction<ServerResponse> routerFunction(){
	RETURN RouterFunctions.route()
	.GET("/uri",customerHanler::loadCus)             //uri -> same as rest api endpoit to call this route/endpoint
	.GET("/uri",anotherHanler::anotherCus)   // here anotherHanler is seperate handler/class and anotherCus() method of that handler. same other enpoints we can add.
}

Note: if we want to send via URI then same as Rest controller {}, but in Handler ir will recieve as a -> request (request.pathVariable (GET) /request.bodyToMono (on POST))

}

Handler:------->>>>>>>>>>>>>
@Service
Class CusHandler{

public Mono<ServerResponse> loadCus(){
	Flux<Customer> list = dao.getCustomer();
	return ServerResponse.ok()
	.contentType(MediaType.TEXT_EVENT_STREAM)  // will tell to  publiser send data in stream instead of Object. 
	.body(list,Customer.class);                // (without this data browser will wait for whole response to complete and then we will see data in brower as whole not in async.)
}

}
------------->>>>>>>>>>>
Exception:----->>>>>>>>>>
class extends AbstractErrorWebExceptionHandler{
override --> RouterFunction<ServerResponse> getRoutingFunction() { -->> add all URL which will use this custom handler. 
   RETURN RouterFunctions.route()
	.GET("/uri",customerHanler::loadCus).........
}



-----------------------------------------------------------------------------------------------------------
----------------------------Testing in springboot-------------------------------------------------------------------------
Unit testing: checking each part individually 
integraton tesing: checking all parts work togethor.

-----------------------@SpringbootTest & @MockBean-------------------------
@SpringbootTest: it is use for integration testing in springboot. IT says "start up the spring context when this test runs"
				We use this when we need to test how diff parts of our app work together. it is great  when we need to know th full behaviour of app.

@MockBean: use when we want the mock/fake version of service/component. this is usefule when we wnat to test a part of app without involving its dependencies.
   for ex : if we are testing a service that is dependent on repo , we can mock the repo to control how it behaves and test the service in isolation.
   
---------------------------ymal or property-----------------------
yml is more readable+ easy to maintain but error porn(due to space). _ use in complex structure.
property is key value and straightforword.

-----------------------------------@Async / @EnableAsync------------------------------------ 
In the Spring framework, asynchronous execution can be achieved using the @Async and @EnableAsync annotations. 

@EnableAsync: It indicates spring boot to enables asynchronous behavior for all that methods which are annotated with @Async.
@Async: This annotation is generally used over the method, which contains the logic related to Asynchronous task, like Sending Email, OTP, Service Messages,
 Etc.. This methods are generally a part of any Service or class which is annotated with @Service or @Component annotation.
 
First, let’s go over the rules. @Async has two limitations:
It must be applied to public methods only
Self-invocation — calling the async method from within the same class — won’t work -->>
	Avoid Calling @Async Methods within the Same Class
	Due to the proxy-based nature of Spring’s AOP framework, if you call an @Async method from another method within the same class, it will not actually execute asynchronously. 
	This is because the call won't go through the proxy but will be a direct method call. Always be aware of this limitation.

By default, 
Spring uses a SimpleAsyncTaskExecutor with default confuguration to run methods asynchronously. However, for production scenarios, it's recommended to configure a custom TaskExecutor.
 so better to create own ThreadPoolTaskExecutor --> no of core, no of queue max pool size as per own need.
 
 public Executor getAsyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(5); //no of thread alive even though thread are ideal.
        executor.setMaxPoolSize(10); // max no of thread that pool can have.
        executor.setQueueCapacity(25); // max no of task kept in queue.
        executor.initialize();
        return executor;
    }
 
Exa: Order -->> INventory -->> Payment -->> Notification -->> vendor --> Packging --> assignVendor -->> delivery
     So if our services run in async then it will take so long, so better to put sync till payment and after that we call all services as a async so user will get notifiy order 
	 placed and recieved track id so that he can track other async steps.
	 



Exception:--------------------->>>>>>>>>>>>>>>

When a method return type is a Future, exception handling is easy. Future.get() method will throw the exception.
However, exceptions will not be propagated to the calling thread if the return type is void. So, we need to add extra configurations to handle exceptions.


class EmailService {
    @Async
    public void sendEmail() throws Exception{
        throw new Exception("Oops, cannot send email!");
    }
}

class PurchaseService {    
    public void purchase(){
        try{
            emailService.sendEmail();
        }catch (Exception e){
            System.out.println("Caught exception: " + e.getMessage());
        }

// In the above code, the exception is thrown in asyncMethod() but it will not be caught by the calling thread and the catch block will not be executed.

To properly handle exceptions in an @Async method, we can use a combination of Future and try-catch blocks. Here’s an example:

class EmailService {
    @Async
    public Future<String> sendEmail() throws Exception{
        throw new Exception("Oops, cannot send email!");
    }
}
class PurchaseService {
    public void purchase(){
        try{
            Future<String> future = emailService.sendEmail();
            String result = future.get();
            System.out.println("Result: " + result);
        }catch (Exception e){
            System.out.println("Caught exception: " + e.getMessage());
        }
//By returning a Future object and using a try-catch block, we can properly handle and catch exceptions thrown within an @Async method.

Note: @Async reuturn type will be Future,compatibleFuture or void only not String or Object.
									OR
We’ll create a custom async exception handler by implementing AsyncUncaughtExceptionHandler interface. 
The handleUncaughtException() method is invoked when there are any uncaught asynchronous exceptions:	
Steps: 
	public class SpringAsyncConfig implements AsyncConfigurer {
	When we implement AsyncConfigurer interface then override 2 methods:
	
	@Override
    public Executor getAsyncExecutor() {  // Create ThreadPoolTaskExecutor
        ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();
        threadPoolTaskExecutor.initialize();
        return threadPoolTaskExecutor;
    }
	
	@Override
	public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {
		return new CustomAsyncExceptionHandler();
	}
	
public class CustomAsyncExceptionHandler implements AsyncUncaughtExceptionHandler {

    @Override
    public void handleUncaughtException(Throwable throwable, Method method, Object... obj) { 
        System.out.println("Exception message - " + throwable.getMessage());
        System.out.println("Method name - " + method.getName());
        for (Object param : obj) {
            System.out.println("Parameter value - " + param);
        }
    }
    
}
	
-----------------------springboot file upload-------------
1- use @PostMaping to create a endpoint
2- add a method that accepts MultipartFIle as a parameters

------------------sending mail of welcome-----------
1- starter-mail  dependency
2- application.yml : set mail server setting.
	spring.mail.host=smtp.gmail.com
	spring.mail.port=587
	spring.mail.username=<Login User to SMTP server>
	spring.mail.password=<Login password to SMTP server>
	spring.mail.properties.mail.smtp.auth=true
	spring.mail.properties.mail.smtp.starttls.enable=true
3 - write a service call that use JavaMailSender to send emails. In this we write the content and use the send method to send.
	import org.springframework.mail.SimpleMailMessage;
    import org.springframework.mail.javamail.JavaMailSender;
	
	SimpleMailMessage mailMessage = new SimpleMailMessage();
	// Setting up necessary details
	mailMessage.setFrom(sender);
	mailMessage.setTo(details.getRecipient());
	mailMessage.setText(details.getMsgBody());
	mailMessage.setSubject(details.getSubject());

	// Sending the mail
	javaMailSender.send(mailMessage);
	
-----------------how springboot security impl-------------
1- starter-spring-security
2- create config class extends WebSecurityConfigurerAdapter to custimize security settings like specify secure endpoints.
3 - Implement UserDetailsService interface to load user info fro DB and use a password encoder like BCryptPassWOrdEncoder for secure password storage.
4 - secure endpoints using @PreAuthorize based on roles and permission.

---------------------------------how to disabel auto configuration class----------------
1 - exclude attribule in @SpringBootApplication ex if we want to exclude DataSourceAutoCOnfiguration class.
  @SpringBootApplication(exclude = {DataSourceAutoCOnfiguration.class})
  
------------------------------------------how to scale springboot spp to handle high traffic-----------
1- add more app instance: horizontal scaling and use load balancer.
2- break your app into micoservice so each can scale independently
3 - Use caching for frequent access data.
4- asynchronous call .

-------------------------------------------CSRF.Disable()----------Cross-Site Request Forgery--------------------------------

Your bank has a website and on this website there's a mechanism to transfer money. You login and transfer a little money. You forget to logout. Then you navigate to an evil website.
The evil website duplicates this form and sends a request to transfer 100 bucks from your account to theirs. They can't see the cookie but the cookies are sent with the request.
 They don't have to see the key in order to turn the key in the lock. Because you forgot to logout, your Session ID is still active (typically they expire but sometimes 
 they don't want to burden their customers with logging in every time).
The bank can't tell who sent the request, and they certainly can't block all malicious websites since there are probably more malicious sites than legitimate ones out there.
So, the bank enables CSRF protection which requires a special token to be generated for the request. The token will be embedded in the request and won't be stored in a cookie.
 Without this token, the request will rejected.
The token is generated by the legitimate site and if you're not navigating from this site's official page, then the token won't be generated. 
The evil website can't generate this token so they cannot forge the transaction.

To answer your question, why would you want to disable this? Here are some common reasons:
You have a different token mechanism already, such as JWT. JWT is typically used because the tokens can be passed to a JavaScript 
	front end framework (such as Angular) which allows your front end to use the endpoints securely.
The endpoint does not actually "change the state of the application". This means it doesn't do anything a malicious attacker could exploit.
You are maintaining boundary security in some other way, such as an appliance that manages an enterprise's security.

-----------------------------------------------------

@Modifying annotation is used when you want to execute modifying queries, such as updates or deletes. 
This annotation is necessary because, by default, Spring Data JPA repository methods are considered read-only and are optimized for querying data, not modifying it. 

// PUT endpoint to update user details based on id
@PutMapping("updateuser/{id}")
public String updateUser(@PathVariable int id) {
	return userService.updateUserDetail(id);
}

@PostMapping("/save")
public Book save(@RequestBody User user) {
}

@DeleteMapping(value = "/user")
public ResponseEntity updateUserData(@RequestParam(value = "userId") Long userId){
}

userService{
updateUserDetail(Long id){
	// get id from controller.
	User user = userRepo.findById(id);
	user.sername("erer");

	userRepository.save(user);	
}

public interface UserRepository extends JpaRepository<User, Integer> {

@Modifying
@Query("update User u set u.name =:name where u.id =:id")
void deactivateUsersNotLoggedInSince(@Param("name") String name, @Param("id") Long id);
void deactivateUsersNotLoggedInSince(String name, Long id);
}

-----------------------------------------------------RequestParam vs PathVariable----------------------
@RequestParam and @PathVariable can both be used to extract values from the request URI, but they are a bit different.

@PathVariables extract values from the URI path:
http://localhost:8080/spring-mvc-basics/foos/abc

	@GetMapping("/foos/{id}")
	@ResponseBody
	public String getFooById(@PathVariable String id) {
		return "ID: " + id;
	}

@RequestParams extract values from the query string
http://localhost:8080/spring-mvc-basics/foos?id=abc
	@GetMapping("/foos")
	@ResponseBody
	public String getFooByIdUsingQueryParam(@RequestParam String id) {
		return "ID: " + id;
	}

Both can be Optional:::::::::::::

@GetMapping({"/myfoos/optional", "/myfoos/optional/{id}"})
@ResponseBody
public String getFooByOptionalId(@PathVariable(required = false) String id){
    return "ID: " + id;
}
http://localhost:8080/spring-mvc-basics/myfoos/optional/abc            -->> ID: abc
  OR
http://localhost:8080/spring-mvc-basics/myfoos/optional                 -->> ID: null

-------------------------------------------------------------------------------------------------------------------

Java 8 has introduced a new class Optional in java.util package. It can help in writing a neat code without using too many null checks.
The Optional class in Java 8 is a container object that may or may not contain a non-null value.

--------------------



Http/rest: TCP Handshake on every request so it make slow and watchlist has many stocks so multiple req to server make it slow.
Web  socket: persistence connection so once connection stable/ one TCP handshake then client sent one req  with watch list ID and get all the watchlist data like current price.
server can send data to client witout asking anythin gfrom client
IN this server sent data without client req so in watchlist case watchlist price should be update without client req. 

zeroda to nse to get price:
1 - we socket.
2- lease line : private line so data continusly comng fro nse to zerodha. those will hit kafka and price will store in DB.

DB : time seriese DB  

BUY sell: for this HTTP no web socket. + validator (check before buy/sell money or price entered etc ) -> order mang service. this send req to nse using webSOcket and 
zerodha works done -> nse 